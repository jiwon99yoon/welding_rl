#------------------------------------------------------------------------------------------------------------------
# video lengthì¡°ì ˆ, env.pyë°”ë€œì— ë”°ë¼ ë˜‘ê°™ì´ ë°”ê¿”ì¤Œ - 20250608 1430
# /home/minjun/rl_ws/src/mujoco_rl_env/train.py
import argparse
import os
import imageio
import numpy as np
from datetime import datetime
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.monitor import Monitor
from mujoco_rl_env.env.franka_reach_env import FrankaReachEnv

class VideoRecorderCallback(BaseCallback):
    """50 iterationë§ˆë‹¤ ë¹„ë””ì˜¤ë¥¼ ì €ì¥í•˜ëŠ” ì»¤ìŠ¤í…€ ì½œë°±"""
    # video length 300ìœ¼ë¡œ ë³€ê²½, record freq 50(iteration) , 100ì¼ë•Œ 4ì´ˆì¯¤
    def __init__(self, eval_env_fn, video_folder="videos", video_length=300, record_freq=50, run_name="", verbose=1):
        super().__init__(verbose)
        self.eval_env_fn = eval_env_fn
        self.video_folder = video_folder
        self.video_length = video_length # 300 steps = 10 seconds @ 30fps
        self.record_freq = record_freq
        self.run_name = run_name
        self.iteration_count = 0
        os.makedirs(video_folder, exist_ok=True)
        
    def _on_step(self) -> bool:
        # ë§¤ 2048 ìŠ¤í…(1 iteration)ë§ˆë‹¤ ì¹´ìš´íŠ¸
        if self.n_calls % 2048 == 0:
            self.iteration_count += 1
            
            # 50 iterationë§ˆë‹¤ ë¹„ë””ì˜¤ ë…¹í™” 
            if self.iteration_count % self.record_freq == 0:
                self._record_video()
                
        return True
    
    def _record_video(self):
        """í˜„ì¬ ì •ì±…ìœ¼ë¡œ ë¹„ë””ì˜¤ ë…¹í™”"""
        if self.verbose > 0:
            print(f"\nğŸ¬ Recording video at iteration {self.iteration_count}...")
        
        # í‰ê°€ìš© í™˜ê²½ ìƒì„± (ì •ê·œí™” ì—†ì´)
        env = self.eval_env_fn()
        
        # VecNormalize ë˜í¼ê°€ ìˆë‹¤ë©´ ì •ê·œí™” í†µê³„ ë™ê¸°í™”
        if hasattr(self.model, 'get_vec_normalize_env'):
            vec_norm = self.model.get_vec_normalize_env()
            if vec_norm is not None:
                eval_env_norm = VecNormalize(DummyVecEnv([lambda: env]), training=False)
                eval_env_norm.obs_rms = vec_norm.obs_rms
                eval_env_norm.ret_rms = vec_norm.ret_rms
                obs = eval_env_norm.reset()
                eval_env = eval_env_norm
            else:
                obs = env.reset()
        else:
            obs = env.reset()
        
        frames = []
        episode_reward = 0
        
        for step in range(self.video_length):
            # í˜„ì¬ ì •ì±…ìœ¼ë¡œ í–‰ë™ ì„ íƒ
            if isinstance(eval_env, VecNormalize):
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, done, info = eval_env.step(action)
                # ì‹¤ì œ í™˜ê²½ì—ì„œ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
                frame = eval_env.venv.envs[0].render()
            else:
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, done, info = env.step(action)
                frame = env.render()
            
            # rewardê°€ ë°°ì—´ì¸ ê²½ìš° ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜
            if isinstance(reward, np.ndarray):
                reward = reward.item()
            episode_reward += reward
            
            if frame is not None:
                frames.append(frame)
            
            # doneë„ ë°°ì—´ì¼ ìˆ˜ ìˆìŒ
            if isinstance(done, np.ndarray):
                done = done.item()
            if done:
                print(f"   ğŸ¯ Goal reached at step {step}!")
                # ëª©í‘œ ë„ë‹¬ í›„ ì¶”ê°€ í”„ë ˆì„ ê¸°ë¡ (ì„±ê³µ í™•ì¸ìš©)
                for _ in range(min(30, self.video_length - step)):  # 1ì´ˆ ë”
                    frame = env.render() if not isinstance(eval_env, VecNormalize) else eval_env.venv.envs[0].render()
                    if frame is not None:
                        frames.append(frame)
                break
        
        # ë¹„ë””ì˜¤ ì €ì¥
        if frames:
            # íŒŒì¼ëª…ì— run_name í¬í•¨
            filename = f"{self.video_folder}/{self.run_name}_iter_{self.iteration_count:04d}.mp4"
            
            # íŒŒì¼ëª… ë™ì¼ - ê²¹ì¹¨ ì´ìŠˆ
            # start_iter = self.iteration_count - 49
            # end_iter = self.iteration_count
            # filename = f"{self.video_folder}/iteration_{start_iter:03d}-{end_iter:03d}.mp4"
            
            imageio.mimsave(filename, frames, fps=30)
            
            if self.verbose > 0:
                print(f"âœ… Video saved: {filename}")
                # episode_rewardê°€ ë°°ì—´ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ floatë¡œ ë³€í™˜
                reward_value = float(episode_reward) if isinstance(episode_reward, (int, float)) else float(episode_reward.item())
                print(f"   Episode reward: {reward_value:.2f}")
                print(f"   Episode length: {len(frames)} frames ({len(frames)/30:.1f} seconds)")
        
        # í™˜ê²½ ì •ë¦¬
        if isinstance(eval_env, VecNormalize):
            eval_env.close()
        else:
            env.close()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Train PPO on the Franka reach environment"
    )
    parser.add_argument(
        "--xml", default="models/fr3_reach.xml",
        help="Path to your MuJoCo XML model"
    )
    parser.add_argument(
        "--init", nargs='*', type=float,
        default=None,  # Noneìœ¼ë¡œ ê¸°ë³¸ê°’ ì„¤ì •
        help="Initial joint positions (optional)"
    )
    parser.add_argument(
        "--goal", nargs='*', type=float,
        default=None,  # Noneìœ¼ë¡œ ê¸°ë³¸ê°’ ì„¤ì •
        help="Goal joint positions (optional, length = model.nq)"
    )
    parser.add_argument(
        "--video-length", type=int, default=300,
        help="Maximum video length in steps (default: 300 = 10 seconds)"
    )
    parser.add_argument(
        "--record-freq", type=int, default=50,
        help="Record video every N iterations (default: 50)"
    )
    args = parser.parse_args()

    # ì‹¤í–‰ ì‹œê°„ ê¸°ë°˜ ê³ ìœ  ID ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_name = f"ppo_franka_{timestamp}"

    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("videos", exist_ok=True)
    os.makedirs("runs/best_model", exist_ok=True)
    os.makedirs("runs/franka_reach", exist_ok=True)

    # init ì¸ì ì²˜ë¦¬
    if args.init is not None and len(args.init) > 0:
        init_qpos = args.init
    else:
        init_qpos = None  # í™˜ê²½ì—ì„œ ê¸°ë³¸ê°’ ì‚¬ìš©
        print("â„¹ï¸  No initial pose provided, using lap_start from XML")

    # goal ì¸ì ì²˜ë¦¬
    if args.goal is not None and len(args.goal)>0:
        goal_qpos = args.goal
        print(f"âœ… Goal joint positions: {goal_qpos}")
    else:
        goal_qpos = None  # í™˜ê²½ì—ì„œ lap_end ì‚¬ìš©
        print("â„¹ï¸  No goal_qpos provided, sampling or using default init as goal")

    # 1) í•™ìŠµìš© í™˜ê²½ ìƒì„± (Monitorë¡œ ê°ì‹¸ê¸°)
    def make_train_env():
        env = FrankaReachEnv(
            xml_path=args.xml,
            init_qpos=init_qpos,
            goal_qpos=goal_qpos,
            render_mode=None
        )
        # Monitorë¡œ ê°ì‹¸ì„œ ì—í”¼ì†Œë“œ í†µê³„ ê¸°ë¡
        env = Monitor(env)
        return env
    
    # 2) í‰ê°€ìš© í™˜ê²½ ìƒì„± í•¨ìˆ˜
    def make_eval_env():
        env = FrankaReachEnv(
            xml_path=args.xml,
            init_qpos=init_qpos,
            goal_qpos=goal_qpos,
            render_mode="rgb_array"
        )
        # í‰ê°€ í™˜ê²½ë„ Monitorë¡œ ê°ì‹¸ê¸°
        env = Monitor(env)
        return env
    
    # í•™ìŠµ í™˜ê²½ ì„¤ì •
    train_env = DummyVecEnv([make_train_env])
    train_env = VecNormalize(
        train_env, 
        norm_obs=True, 
        norm_reward=False,
        clip_obs=10.0
    )
    
    # í‰ê°€ í™˜ê²½ ì„¤ì • (ì •ê·œí™” í¬í•¨)
    eval_env = DummyVecEnv([make_eval_env])
    eval_env = VecNormalize(
        eval_env,
        norm_obs=True,
        norm_reward=False,
        training=False,  # í‰ê°€ì‹œì—ëŠ” í†µê³„ ì—…ë°ì´íŠ¸ ì•ˆí•¨
        clip_obs=10.0
    )
    
    # 3) ì½œë°± ì„¤ì •
    # EvalCallback - TensorBoardì— eval ë©”íŠ¸ë¦­ ê¸°ë¡
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=f"runs/{run_name}/best_model",
        log_path=f"runs/{run_name}/logs",
        eval_freq=args.record_freq * 2048,  # 50 iterationë§ˆë‹¤
        n_eval_episodes=5,     # 5 ì—í”¼ì†Œë“œ í‰ê· 
        deterministic=True,
        verbose=1
    )
    
    # VideoRecorderCallback - ë¹„ë””ì˜¤ ë…¹í™”
    video_callback = VideoRecorderCallback(
        eval_env_fn=make_eval_env,
        video_folder=f"videos/{run_name}",
        video_length=args.video_length,
        record_freq=args.record_freq,
        run_name=run_name,
        verbose=1
    )
    
    # ì½œë°± ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ê¸°
    callbacks = CallbackList([eval_callback, video_callback])
    
    # 4) PPO ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
    model = PPO(
        "MlpPolicy",
        train_env,
        learning_rate=3e-4, #ê¸°ì¡´ 3e-4
        n_steps=2048,
        batch_size=64,
        n_epochs=20, #ê¸°ì¡´ 10
        gamma=0.995,# ê¸°ì¡´ 0.99 
        gae_lambda=0.95,
        clip_range=0.3, #ê¸°ì¡´ 0.2
        ent_coef=0.05,
        vf_coef=0.3, # ê¸°ì¡´ 0.5
        max_grad_norm=0.7, #ê¸°ì¡´ 0.5
        verbose=1,
        tensorboard_log=f"runs/{run_name}/tensorboard"
    )
    
    print("="*60)
    print("PPO Training on Franka Reach Environment: {run_name}")
    print(f"- Initial pose: {init_qpos if init_qpos else 'lap_start (from XML)'}")
    print(f"- Goal pose: {goal_qpos if goal_qpos else 'randomly sampled or init'}")
    print(f"- Recording video every {args.record_freq} iterations")
    print(f"- Evaluating every 50 iterations (5 episodes)")
    print(f"- Video length: {args.video_length} steps ({args.video_length/30:.1f} seconds)")
    print(f"- Videos will be saved to: videos/{run_name}/")
    print(f"- TensorBoard logs: runs/{run_name}/tensorboard")
    print("="*60)
    
    # ì •ê·œí™” í†µê³„ ë™ê¸°í™”
    eval_env.obs_rms = train_env.obs_rms
    eval_env.ret_rms = train_env.ret_rms
    
    try:
        model.learn(
            total_timesteps=1_000_000, #timestep 1000000ê¹Œì§€
            callback=callbacks,
            log_interval=10,
            tb_log_name=run_name # TensorBoard ë¡œê·¸ ì´ë¦„
        )
        
        # ëª¨ë¸ ë° ì •ê·œí™” í†µê³„ ì €ì¥
        model.save(f"runs/{run_name}/final_model")
        train_env.save(f"runs/{run_name}/vec_normalize.pkl")
        
        print(f"\nâœ… Training completed successfully!")
        print(f"ğŸ”¸ Run name: {run_name}")
        print(f"ğŸ”¸ Final model: runs/{run_name}/final_model.zip")
        print(f"ğŸ”¸ Best model: runs/{run_name}/best_model/best_model.zip")
        print(f"ğŸ”¸ Normalization stats: runs/{run_name}/vec_normalize.pkl")
        
        # ì €ì¥ëœ ë¹„ë””ì˜¤ ëª©ë¡ ì¶œë ¥
        videos = sorted([f for f in os.listdir(f"videos/{run_name}") if f.endswith('.mp4')])
        print(f"\nğŸ“¹ Saved videos ({len(videos)} total):")
        for video in videos[-5:]:  # ë§ˆì§€ë§‰ 5ê°œë§Œ í‘œì‹œ
            print(f"   - {video}")
        if len(videos) > 5:
            print(f"   ... and {len(videos)-5} more")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  Training interrupted by user")
        model.save(f"runs/{run_name}/interrupted_model")
        
    except Exception as e:
        print(f"\nâŒ Error during training: {e}")
        raise
        
    finally:
        train_env.close()
        eval_env.close()
        print(f"\nğŸ’¡ To monitor training progress, run:")
        print(f"   tensorboard --logdir runs/{run_name}/tensorboard")



# ------------------------------------------------------------------------------
# /home/minjun/rl_ws/src/mujoco_rl_env/train.py
# import argparse
# import os
# import imageio
# import numpy as np
# from datetime import datetime
# from stable_baselines3 import PPO
# from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList
# from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
# from stable_baselines3.common.monitor import Monitor
# from mujoco_rl_env.env.franka_reach_env import FrankaReachEnv

# class VideoRecorderCallback(BaseCallback):
#     """ë¹„ë””ì˜¤ë¥¼ ì €ì¥í•˜ëŠ” ì»¤ìŠ¤í…€ ì½œë°±"""
#     def __init__(self, eval_env_fn, video_folder="videos", video_length=300, 
#                  record_freq=50, run_name="", verbose=1):
#         super().__init__(verbose)
#         self.eval_env_fn = eval_env_fn
#         self.video_folder = video_folder
#         self.video_length = video_length  # 300 steps = 10 seconds @ 30fps
#         self.record_freq = record_freq
#         self.run_name = run_name
#         self.iteration_count = 0
#         os.makedirs(video_folder, exist_ok=True)
        
#     def _on_step(self) -> bool:
#         # ë§¤ 2048 ìŠ¤í…(1 iteration)ë§ˆë‹¤ ì¹´ìš´íŠ¸
#         if self.n_calls % 2048 == 0:
#             self.iteration_count += 1
            
#             # record_freq iterationë§ˆë‹¤ ë¹„ë””ì˜¤ ë…¹í™”
#             if self.iteration_count % self.record_freq == 0:
#                 self._record_video()
                
#         return True
    
#     def _record_video(self):
#         """í˜„ì¬ ì •ì±…ìœ¼ë¡œ ë¹„ë””ì˜¤ ë…¹í™”"""
#         if self.verbose > 0:
#             print(f"\nğŸ¬ Recording video at iteration {self.iteration_count}...")
        
#         # í‰ê°€ìš© í™˜ê²½ ìƒì„±
#         env = self.eval_env_fn()
        
#         # VecNormalize ë˜í¼ê°€ ìˆë‹¤ë©´ ì •ê·œí™” í†µê³„ ë™ê¸°í™”
#         if hasattr(self.model, 'get_vec_normalize_env'):
#             vec_norm = self.model.get_vec_normalize_env()
#             if vec_norm is not None:
#                 eval_env_norm = VecNormalize(DummyVecEnv([lambda: env]), training=False)
#                 eval_env_norm.obs_rms = vec_norm.obs_rms
#                 eval_env_norm.ret_rms = vec_norm.ret_rms
#                 obs = eval_env_norm.reset()
#                 eval_env = eval_env_norm
#             else:
#                 obs = env.reset()
#         else:
#             obs = env.reset()
        
#         frames = []
#         episode_reward = 0
        
#         for step in range(self.video_length):
#             # í˜„ì¬ ì •ì±…ìœ¼ë¡œ í–‰ë™ ì„ íƒ
#             if isinstance(eval_env, VecNormalize):
#                 action, _ = self.model.predict(obs, deterministic=True)
#                 obs, reward, done, info = eval_env.step(action)
#                 frame = eval_env.venv.envs[0].render()
#             else:
#                 action, _ = self.model.predict(obs, deterministic=True)
#                 obs, reward, done, info = env.step(action)
#                 frame = env.render()
            
#             # rewardê°€ ë°°ì—´ì¸ ê²½ìš° ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜
#             if isinstance(reward, np.ndarray):
#                 reward = reward.item()
#             episode_reward += reward
            
#             if frame is not None:
#                 frames.append(frame)
            
#             # doneë„ ë°°ì—´ì¼ ìˆ˜ ìˆìŒ
#             if isinstance(done, np.ndarray):
#                 done = done.item()
#             if done:
#                 print(f"   ğŸ¯ Goal reached at step {step}!")
#                 # ëª©í‘œ ë„ë‹¬ í›„ ì¶”ê°€ í”„ë ˆì„ ê¸°ë¡ (ì„±ê³µ í™•ì¸ìš©)
#                 for _ in range(min(30, self.video_length - step)):  # 1ì´ˆ ë”
#                     frame = env.render() if not isinstance(eval_env, VecNormalize) else eval_env.venv.envs[0].render()
#                     if frame is not None:
#                         frames.append(frame)
#                 break
        
#         # ë¹„ë””ì˜¤ ì €ì¥
#         if frames:
#             # íŒŒì¼ëª…ì— run_name í¬í•¨
#             filename = f"{self.video_folder}/{self.run_name}_iter_{self.iteration_count:04d}.mp4"
            
#             imageio.mimsave(filename, frames, fps=30)
            
#             if self.verbose > 0:
#                 print(f"âœ… Video saved: {filename}")
#                 reward_value = float(episode_reward) if isinstance(episode_reward, (int, float)) else float(episode_reward.item())
#                 print(f"   Episode reward: {reward_value:.2f}")
#                 print(f"   Episode length: {len(frames)} frames ({len(frames)/30:.1f} seconds)")
        
#         # í™˜ê²½ ì •ë¦¬
#         if isinstance(eval_env, VecNormalize):
#             eval_env.close()
#         else:
#             env.close()

# if __name__ == "__main__":
#     parser = argparse.ArgumentParser(
#         description="Train PPO on the Franka reach environment"
#     )
#     parser.add_argument(
#         "--xml", default="models/fr3_reach.xml",
#         help="Path to your MuJoCo XML model"
#     )
#     parser.add_argument(
#         "--init", nargs='*', type=float,
#         default=None,
#         help="Initial joint positions (optional)"
#     )
#     parser.add_argument(
#         "--goal", nargs=7, type=float,
#         default=None,
#         help="Goal joint positions for 7 DOF arm"
#     )
#     parser.add_argument(
#         "--video-length", type=int, default=300,
#         help="Maximum video length in steps (default: 300 = 10 seconds)"
#     )
#     parser.add_argument(
#         "--record-freq", type=int, default=50,
#         help="Record video every N iterations (default: 50)"
#     )
#     args = parser.parse_args()

#     # ì‹¤í–‰ ì‹œê°„ ê¸°ë°˜ ê³ ìœ  ID ìƒì„±
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     run_name = f"ppo_franka_{timestamp}"
    
#     # ë””ë ‰í† ë¦¬ ìƒì„±
#     os.makedirs(f"videos/{run_name}", exist_ok=True)
#     os.makedirs(f"runs/{run_name}/best_model", exist_ok=True)
#     os.makedirs(f"runs/{run_name}/logs", exist_ok=True)

#     # init ì¸ì ì²˜ë¦¬
#     if args.init is not None and len(args.init) > 0:
#         init_qpos = args.init
#     else:
#         init_qpos = None
#         print("â„¹ï¸  No initial pose provided, will try to use lap_start keyframe from XML")

#     # goal ì¸ì ì²˜ë¦¬ (joint positions)
#     if args.goal is not None:
#         goal_qpos = args.goal
#         print(f"âœ… Goal joint positions: {goal_qpos}")
#     else:
#         goal_qpos = None
#         print("â„¹ï¸  No goal joint positions provided")

#     # 1) í•™ìŠµìš© í™˜ê²½ ìƒì„±
#     def make_train_env():
#         env = FrankaReachEnv(
#             xml_path=args.xml,
#             init_qpos=init_qpos,
#             goal_qpos=goal_qpos,
#             render_mode=None
#         )
#         env = Monitor(env, f"runs/{run_name}/logs")
#         return env
    
#     # 2) í‰ê°€ìš© í™˜ê²½ ìƒì„± í•¨ìˆ˜
#     def make_eval_env():
#         env = FrankaReachEnv(
#             xml_path=args.xml,
#             init_qpos=init_qpos,
#             goal_qpos=goal_qpos,
#             render_mode="rgb_array"
#         )
#         env = Monitor(env)
#         return env
    
#     # í•™ìŠµ í™˜ê²½ ì„¤ì •
#     train_env = DummyVecEnv([make_train_env])
#     train_env = VecNormalize(
#         train_env, 
#         norm_obs=True, 
#         norm_reward=True,  # reward normalization í™œì„±í™”
#         clip_obs=10.0,
#         clip_reward=10.0
#     )
    
#     # í‰ê°€ í™˜ê²½ ì„¤ì •
#     eval_env = DummyVecEnv([make_eval_env])
#     eval_env = VecNormalize(
#         eval_env,
#         norm_obs=True,
#         norm_reward=False,
#         training=False,
#         clip_obs=10.0
#     )
    
#     # 3) ì½œë°± ì„¤ì •
#     # EvalCallback
#     eval_callback = EvalCallback(
#         eval_env,
#         best_model_save_path=f"runs/{run_name}/best_model",
#         log_path=f"runs/{run_name}/logs",
#         eval_freq=args.record_freq * 2048,  # record_freq iterationsë§ˆë‹¤
#         n_eval_episodes=5,
#         deterministic=True,
#         verbose=1
#     )
    
#     # VideoRecorderCallback
#     video_callback = VideoRecorderCallback(
#         eval_env_fn=make_eval_env,
#         video_folder=f"videos/{run_name}",
#         video_length=args.video_length,
#         record_freq=args.record_freq,
#         run_name=run_name,
#         verbose=1
#     )
    
#     callbacks = CallbackList([eval_callback, video_callback])
    
#     # 4) PPO ëª¨ë¸ ìƒì„±
#     model = PPO(
#         "MlpPolicy",
#         train_env,
#         learning_rate=3e-4,
#         n_steps=2048,
#         batch_size=64,
#         n_epochs=10,
#         gamma=0.99,
#         gae_lambda=0.95,
#         clip_range=0.2,
#         ent_coef=0.01,
#         vf_coef=0.5,
#         max_grad_norm=0.5,
#         verbose=1,
#         tensorboard_log=f"runs/{run_name}/tensorboard"
#     )
    
#     print("="*60)
#     print(f"PPO Training: {run_name}")
#     print(f"- Initial pose: {init_qpos if init_qpos else 'zeros'}")
#     print(f"- Goal joints: {goal_qpos if goal_qpos else 'same as initial'}")
#     print(f"- Recording video every {args.record_freq} iterations")
#     print(f"- Video length: {args.video_length} steps ({args.video_length/30:.1f} seconds)")
#     print(f"- Videos will be saved to: videos/{run_name}/")
#     print(f"- TensorBoard logs: runs/{run_name}/tensorboard")
#     print("="*60)
    
#     # ì •ê·œí™” í†µê³„ ë™ê¸°í™”
#     eval_env.obs_rms = train_env.obs_rms
#     eval_env.ret_rms = train_env.ret_rms
    
#     try:
#         model.learn(
#             total_timesteps=500_000,
#             callback=callbacks,
#             log_interval=10,
#             tb_log_name=run_name
#         )
        
#         # ëª¨ë¸ ë° ì •ê·œí™” í†µê³„ ì €ì¥
#         model.save(f"runs/{run_name}/final_model")
#         train_env.save(f"runs/{run_name}/vec_normalize.pkl")
        
#         print(f"\nâœ… Training completed successfully!")
#         print(f"ğŸ”¸ Run name: {run_name}")
#         print(f"ğŸ”¸ Final model: runs/{run_name}/final_model.zip")
#         print(f"ğŸ”¸ Best model: runs/{run_name}/best_model/best_model.zip")
#         print(f"ğŸ”¸ Normalization stats: runs/{run_name}/vec_normalize.pkl")
        
#         # ì €ì¥ëœ ë¹„ë””ì˜¤ ëª©ë¡ ì¶œë ¥
#         videos = sorted([f for f in os.listdir(f"videos/{run_name}") if f.endswith('.mp4')])
#         print(f"\nğŸ“¹ Saved videos ({len(videos)} total):")
#         for video in videos[-5:]:  # ë§ˆì§€ë§‰ 5ê°œë§Œ í‘œì‹œ
#             print(f"   - {video}")
#         if len(videos) > 5:
#             print(f"   ... and {len(videos)-5} more")
        
#     except KeyboardInterrupt:
#         print("\nâš ï¸  Training interrupted by user")
#         model.save(f"runs/{run_name}/interrupted_model")
        
#     except Exception as e:
#         print(f"\nâŒ Error during training: {e}")
#         raise
        
#     finally:
#         train_env.close()
#         eval_env.close()
#         print(f"\nğŸ’¡ To monitor training progress, run:")
#         print(f"   tensorboard --logdir runs/{run_name}/tensorboard")




#------------------------------------------------------------------------------------------------------------------
# 20250608 1717 ìˆ˜ì •
# /home/minjun/rl_ws/src/mujoco_rl_env/train.py
# import argparse
# import os
# import imageio
# import numpy as np
# from datetime import datetime
# from stable_baselines3 import PPO
# from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList
# from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
# from stable_baselines3.common.monitor import Monitor
# from mujoco_rl_env.env.franka_reach_env import FrankaReachEnv

# class VideoRecorderCallback(BaseCallback):
#     """ë¹„ë””ì˜¤ë¥¼ ì €ì¥í•˜ëŠ” ì»¤ìŠ¤í…€ ì½œë°±"""
#     def __init__(self, eval_env_fn, video_folder="videos", video_length=300, 
#                  record_freq=50, run_name="", verbose=1):
#         super().__init__(verbose)
#         self.eval_env_fn = eval_env_fn
#         self.video_folder = video_folder
#         self.video_length = video_length  # 300 steps = 10 seconds @ 30fps
#         self.record_freq = record_freq
#         self.run_name = run_name
#         self.iteration_count = 0
#         os.makedirs(video_folder, exist_ok=True)
        
#     def _on_step(self) -> bool:
#         # ë§¤ 2048 ìŠ¤í…(1 iteration)ë§ˆë‹¤ ì¹´ìš´íŠ¸
#         if self.n_calls % 2048 == 0:
#             self.iteration_count += 1
            
#             # record_freq iterationë§ˆë‹¤ ë¹„ë””ì˜¤ ë…¹í™”
#             if self.iteration_count % self.record_freq == 0:
#                 self._record_video()
                
#         return True
    
#     def _record_video(self):
#         """í˜„ì¬ ì •ì±…ìœ¼ë¡œ ë¹„ë””ì˜¤ ë…¹í™”"""
#         if self.verbose > 0:
#             print(f"\nğŸ¬ Recording video at iteration {self.iteration_count}...")
        
#         # í‰ê°€ìš© í™˜ê²½ ìƒì„±
#         env = self.eval_env_fn()
        
#         # VecNormalize ë˜í¼ê°€ ìˆë‹¤ë©´ ì •ê·œí™” í†µê³„ ë™ê¸°í™”
#         if hasattr(self.model, 'get_vec_normalize_env'):
#             vec_norm = self.model.get_vec_normalize_env()
#             if vec_norm is not None:
#                 eval_env_norm = VecNormalize(DummyVecEnv([lambda: env]), training=False)
#                 eval_env_norm.obs_rms = vec_norm.obs_rms
#                 eval_env_norm.ret_rms = vec_norm.ret_rms
#                 obs = eval_env_norm.reset()
#                 eval_env = eval_env_norm
#             else:
#                 obs = env.reset()
#         else:
#             obs = env.reset()
        
#         frames = []
#         episode_reward = 0
        
#         for step in range(self.video_length):
#             # í˜„ì¬ ì •ì±…ìœ¼ë¡œ í–‰ë™ ì„ íƒ
#             if isinstance(eval_env, VecNormalize):
#                 action, _ = self.model.predict(obs, deterministic=True)
#                 obs, reward, done, info = eval_env.step(action)
#                 frame = eval_env.venv.envs[0].render()
#             else:
#                 action, _ = self.model.predict(obs, deterministic=True)
#                 obs, reward, done, info = env.step(action)
#                 frame = env.render()
            
#             # rewardê°€ ë°°ì—´ì¸ ê²½ìš° ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜
#             if isinstance(reward, np.ndarray):
#                 reward = reward.item()
#             episode_reward += reward
            
#             if frame is not None:
#                 frames.append(frame)
            
#             # doneë„ ë°°ì—´ì¼ ìˆ˜ ìˆìŒ
#             if isinstance(done, np.ndarray):
#                 done = done.item()
#             if done:
#                 print(f"   ğŸ¯ Goal reached at step {step}!")
#                 # ëª©í‘œ ë„ë‹¬ í›„ ì¶”ê°€ í”„ë ˆì„ ê¸°ë¡ (ì„±ê³µ í™•ì¸ìš©)
#                 for _ in range(min(30, self.video_length - step)):  # 1ì´ˆ ë”
#                     frame = env.render() if not isinstance(eval_env, VecNormalize) else eval_env.venv.envs[0].render()
#                     if frame is not None:
#                         frames.append(frame)
#                 break
        
#         # ë¹„ë””ì˜¤ ì €ì¥
#         if frames:
#             # íŒŒì¼ëª…ì— run_name í¬í•¨
#             filename = f"{self.video_folder}/{self.run_name}_iter_{self.iteration_count:04d}.mp4"
            
#             imageio.mimsave(filename, frames, fps=30)
            
#             if self.verbose > 0:
#                 print(f"âœ… Video saved: {filename}")
#                 reward_value = float(episode_reward) if isinstance(episode_reward, (int, float)) else float(episode_reward.item())
#                 print(f"   Episode reward: {reward_value:.2f}")
#                 print(f"   Episode length: {len(frames)} frames ({len(frames)/30:.1f} seconds)")
        
#         # í™˜ê²½ ì •ë¦¬
#         if isinstance(eval_env, VecNormalize):
#             eval_env.close()
#         else:
#             env.close()

# if __name__ == "__main__":
#     parser = argparse.ArgumentParser(
#         description="Train PPO on the Franka reach environment"
#     )
#     parser.add_argument(
#         "--xml", default="models/fr3_reach.xml",
#         help="Path to your MuJoCo XML model"
#     )
#     parser.add_argument(
#         "--init", nargs='*', type=float,
#         default=None,
#         help="Initial joint positions (optional)"
#     )
#     parser.add_argument(
#         "--goal", nargs=7, type=float,
#         default=None,
#         help="Goal pose: x y z qx qy qz qw"
#     )
#     parser.add_argument(
#         "--video-length", type=int, default=300,
#         help="Maximum video length in steps (default: 300 = 10 seconds)"
#     )
#     parser.add_argument(
#         "--record-freq", type=int, default=50,
#         help="Record video every N iterations (default: 50)"
#     )
#     args = parser.parse_args()

#     # ì‹¤í–‰ ì‹œê°„ ê¸°ë°˜ ê³ ìœ  ID ìƒì„±
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     run_name = f"ppo_franka_{timestamp}"
    
#     # ë””ë ‰í† ë¦¬ ìƒì„±
#     os.makedirs(f"videos/{run_name}", exist_ok=True)
#     os.makedirs(f"runs/{run_name}/best_model", exist_ok=True)
#     os.makedirs(f"runs/{run_name}/logs", exist_ok=True)

#     # init ì¸ì ì²˜ë¦¬
#     if args.init is not None and len(args.init) > 0:
#         init_qpos = args.init
#     else:
#         init_qpos = None
#         print("â„¹ï¸  No initial pose provided, will try to use lap_start keyframe from XML")

#     # goal ì¸ì ì²˜ë¦¬
#     if args.goal is not None:
#         goal_pose = args.goal
#     else:
#         goal_pose = None
#         print("â„¹ï¸  No goal pose provided, using lap_end from XML")

#     # 1) í•™ìŠµìš© í™˜ê²½ ìƒì„±
#     def make_train_env():
#         env = FrankaReachEnv(
#             xml_path=args.xml,
#             init_qpos=init_qpos,
#             goal_pose=goal_pose,
#             render_mode=None
#         )
#         env = Monitor(env, f"runs/{run_name}/logs")
#         return env
    
#     # 2) í‰ê°€ìš© í™˜ê²½ ìƒì„± í•¨ìˆ˜
#     def make_eval_env():
#         env = FrankaReachEnv(
#             xml_path=args.xml,
#             init_qpos=init_qpos,
#             goal_pose=goal_pose,
#             render_mode="rgb_array"
#         )
#         env = Monitor(env)
#         return env
    
#     # í•™ìŠµ í™˜ê²½ ì„¤ì •
#     train_env = DummyVecEnv([make_train_env])
#     train_env = VecNormalize(
#         train_env, 
#         norm_obs=True, 
#         norm_reward=True,  # reward normalization í™œì„±í™”
#         clip_obs=10.0,
#         clip_reward=10.0
#     )
    
#     # í‰ê°€ í™˜ê²½ ì„¤ì •
#     eval_env = DummyVecEnv([make_eval_env])
#     eval_env = VecNormalize(
#         eval_env,
#         norm_obs=True,
#         norm_reward=False,
#         training=False,
#         clip_obs=10.0
#     )
    
#     # 3) ì½œë°± ì„¤ì •
#     # EvalCallback
#     eval_callback = EvalCallback(
#         eval_env,
#         best_model_save_path=f"runs/{run_name}/best_model",
#         log_path=f"runs/{run_name}/logs",
#         eval_freq=args.record_freq * 2048,  # record_freq iterationsë§ˆë‹¤
#         n_eval_episodes=5,
#         deterministic=True,
#         verbose=1
#     )
    
#     # VideoRecorderCallback
#     video_callback = VideoRecorderCallback(
#         eval_env_fn=make_eval_env,
#         video_folder=f"videos/{run_name}",
#         video_length=args.video_length,
#         record_freq=args.record_freq,
#         run_name=run_name,
#         verbose=1
#     )
    
#     callbacks = CallbackList([eval_callback, video_callback])
    
#     # 4) PPO ëª¨ë¸ ìƒì„±
#     model = PPO(
#         "MlpPolicy",
#         train_env,
#         learning_rate=3e-4,
#         n_steps=2048,
#         batch_size=64,
#         n_epochs=10,
#         gamma=0.99,
#         gae_lambda=0.95,
#         clip_range=0.2,
#         ent_coef=0.01,
#         vf_coef=0.5,
#         max_grad_norm=0.5,
#         verbose=1,
#         tensorboard_log=f"runs/{run_name}/tensorboard"
#     )
    
#     print("="*60)
#     print(f"PPO Training: {run_name}")
#     print(f"- Initial pose: {init_qpos if init_qpos else 'lap_start keyframe from XML'}")
#     print(f"- Goal pose: {goal_pose if goal_pose else 'lap_end site from XML'}")
#     print(f"- Recording video every {args.record_freq} iterations")
#     print(f"- Video length: {args.video_length} steps ({args.video_length/30:.1f} seconds)")
#     print(f"- Videos will be saved to: videos/{run_name}/")
#     print(f"- TensorBoard logs: runs/{run_name}/tensorboard")
#     print("="*60)
    
#     # ì •ê·œí™” í†µê³„ ë™ê¸°í™”
#     eval_env.obs_rms = train_env.obs_rms
#     eval_env.ret_rms = train_env.ret_rms
    
#     try:
#         model.learn(
#             total_timesteps=500_000,
#             callback=callbacks,
#             log_interval=10,
#             tb_log_name=run_name
#         )
        
#         # ëª¨ë¸ ë° ì •ê·œí™” í†µê³„ ì €ì¥
#         model.save(f"runs/{run_name}/final_model")
#         train_env.save(f"runs/{run_name}/vec_normalize.pkl")
        
#         print(f"\nâœ… Training completed successfully!")
#         print(f"ğŸ”¸ Run name: {run_name}")
#         print(f"ğŸ”¸ Final model: runs/{run_name}/final_model.zip")
#         print(f"ğŸ”¸ Best model: runs/{run_name}/best_model/best_model.zip")
#         print(f"ğŸ”¸ Normalization stats: runs/{run_name}/vec_normalize.pkl")
        
#         # ì €ì¥ëœ ë¹„ë””ì˜¤ ëª©ë¡ ì¶œë ¥
#         videos = sorted([f for f in os.listdir(f"videos/{run_name}") if f.endswith('.mp4')])
#         print(f"\nğŸ“¹ Saved videos ({len(videos)} total):")
#         for video in videos[-5:]:  # ë§ˆì§€ë§‰ 5ê°œë§Œ í‘œì‹œ
#             print(f"   - {video}")
#         if len(videos) > 5:
#             print(f"   ... and {len(videos)-5} more")
        
#     except KeyboardInterrupt:
#         print("\nâš ï¸  Training interrupted by user")
#         model.save(f"runs/{run_name}/interrupted_model")
        
#     except Exception as e:
#         print(f"\nâŒ Error during training: {e}")
#         raise
        
#     finally:
#         train_env.close()
#         eval_env.close()
#         print(f"\nğŸ’¡ To monitor training progress, run:")
#         print(f"   tensorboard --logdir runs/{run_name}/tensorboard")




#------------------------------------------------------------------------------------------------------------------
# 2025.06.08 13:13 ì˜ìƒ ìƒì„±ê¹Œì§€ ë‹¤ë¨. ê·¼ë° 50ì—ì„œ ë©ˆì¶¤!
# import argparse
# import os
# import imageio
# import numpy as np
# from stable_baselines3 import PPO
# from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList
# from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
# from stable_baselines3.common.monitor import Monitor
# from mujoco_rl_env.env.franka_reach_env import FrankaReachEnv

# class VideoRecorderCallback(BaseCallback):
#     """50 iterationë§ˆë‹¤ ë¹„ë””ì˜¤ë¥¼ ì €ì¥í•˜ëŠ” ì»¤ìŠ¤í…€ ì½œë°±"""
#     def __init__(self, eval_env_fn, video_folder="videos", video_length=100, verbose=1):
#         super().__init__(verbose)
#         self.eval_env_fn = eval_env_fn
#         self.video_folder = video_folder
#         self.video_length = video_length
#         self.iteration_count = 0
#         os.makedirs(video_folder, exist_ok=True)
        
#     def _on_step(self) -> bool:
#         # ë§¤ 2048 ìŠ¤í…(1 iteration)ë§ˆë‹¤ ì¹´ìš´íŠ¸
#         if self.n_calls % 2048 == 0:
#             self.iteration_count += 1
            
#             # 50 iterationë§ˆë‹¤ ë¹„ë””ì˜¤ ë…¹í™”
#             if self.iteration_count % 50 == 0:
#                 self._record_video()
                
#         return True
    
#     def _record_video(self):
#         """í˜„ì¬ ì •ì±…ìœ¼ë¡œ ë¹„ë””ì˜¤ ë…¹í™”"""
#         if self.verbose > 0:
#             print(f"\nğŸ¬ Recording video at iteration {self.iteration_count}...")
        
#         # í‰ê°€ìš© í™˜ê²½ ìƒì„± (ì •ê·œí™” ì—†ì´)
#         env = self.eval_env_fn()
        
#         # VecNormalize ë˜í¼ê°€ ìˆë‹¤ë©´ ì •ê·œí™” í†µê³„ ë™ê¸°í™”
#         if hasattr(self.model, 'get_vec_normalize_env'):
#             vec_norm = self.model.get_vec_normalize_env()
#             if vec_norm is not None:
#                 eval_env_norm = VecNormalize(DummyVecEnv([lambda: env]), training=False)
#                 eval_env_norm.obs_rms = vec_norm.obs_rms
#                 eval_env_norm.ret_rms = vec_norm.ret_rms
#                 obs = eval_env_norm.reset()
#                 eval_env = eval_env_norm
#             else:
#                 obs = env.reset()
#         else:
#             obs = env.reset()
        
#         frames = []
#         episode_reward = 0
        
#         for step in range(self.video_length):
#             # í˜„ì¬ ì •ì±…ìœ¼ë¡œ í–‰ë™ ì„ íƒ
#             if isinstance(eval_env, VecNormalize):
#                 action, _ = self.model.predict(obs, deterministic=True)
#                 obs, reward, done, info = eval_env.step(action)
#                 # ì‹¤ì œ í™˜ê²½ì—ì„œ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
#                 frame = eval_env.venv.envs[0].render()
#             else:
#                 action, _ = self.model.predict(obs, deterministic=True)
#                 obs, reward, done, info = env.step(action)
#                 frame = env.render()
            
#             episode_reward += reward
            
#             if frame is not None:
#                 frames.append(frame)
            
#             if done:
#                 break
        
#         # ë¹„ë””ì˜¤ ì €ì¥
#         if frames:
#             start_iter = self.iteration_count - 49
#             end_iter = self.iteration_count
#             filename = f"{self.video_folder}/iteration_{start_iter:03d}-{end_iter:03d}.mp4"
            
#             imageio.mimsave(filename, frames, fps=30)
            
#             if self.verbose > 0:
#                 print(f"âœ… Video saved: {filename}")
#                 print(f"   Episode reward: {episode_reward:.2f}")
#                 print(f"   Episode length: {len(frames)}")
        
#         # í™˜ê²½ ì •ë¦¬
#         if isinstance(eval_env, VecNormalize):
#             eval_env.close()
#         else:
#             env.close()

# if __name__ == "__main__":
#     parser = argparse.ArgumentParser(
#         description="Train PPO on the Franka reach environment"
#     )
#     parser.add_argument(
#         "--xml", default="models/fr3_reach.xml",
#         help="Path to your MuJoCo XML model"
#     )
#     parser.add_argument(
#         "--init", nargs='*', type=float,
#         default=None,  # Noneìœ¼ë¡œ ê¸°ë³¸ê°’ ì„¤ì •
#         help="Initial joint positions (optional)"
#     )
#     parser.add_argument(
#         "--goal", nargs=7, type=float,
#         default=None,  # Noneìœ¼ë¡œ ê¸°ë³¸ê°’ ì„¤ì •
#         help="Goal pose: x y z qx qy qz qw"
#     )
#     args = parser.parse_args()

#     # ë””ë ‰í† ë¦¬ ìƒì„±
#     os.makedirs("videos", exist_ok=True)
#     os.makedirs("runs/best_model", exist_ok=True)
#     os.makedirs("runs/franka_reach", exist_ok=True)

#     # init ì¸ì ì²˜ë¦¬
#     if args.init is not None and len(args.init) > 0:
#         init_qpos = args.init
#     else:
#         init_qpos = None  # í™˜ê²½ì—ì„œ ê¸°ë³¸ê°’ ì‚¬ìš©
#         print("â„¹ï¸  No initial pose provided, using lap_start from XML")

#     # goal ì¸ì ì²˜ë¦¬
#     if args.goal is not None:
#         goal_pose = args.goal
#     else:
#         goal_pose = None  # í™˜ê²½ì—ì„œ lap_end ì‚¬ìš©
#         print("â„¹ï¸  No goal pose provided, using lap_end from XML")

#     # 1) í•™ìŠµìš© í™˜ê²½ ìƒì„± (Monitorë¡œ ê°ì‹¸ê¸°)
#     def make_train_env():
#         env = FrankaReachEnv(
#             xml_path=args.xml,
#             init_qpos=init_qpos,
#             goal_pose=goal_pose,
#             render_mode=None
#         )
#         # Monitorë¡œ ê°ì‹¸ì„œ ì—í”¼ì†Œë“œ í†µê³„ ê¸°ë¡
#         env = Monitor(env)
#         return env
    
#     # 2) í‰ê°€ìš© í™˜ê²½ ìƒì„± í•¨ìˆ˜
#     def make_eval_env():
#         env = FrankaReachEnv(
#             xml_path=args.xml,
#             init_qpos=init_qpos,
#             goal_pose=goal_pose,
#             render_mode="rgb_array"
#         )
#         # í‰ê°€ í™˜ê²½ë„ Monitorë¡œ ê°ì‹¸ê¸°
#         env = Monitor(env)
#         return env
    
#     # í•™ìŠµ í™˜ê²½ ì„¤ì •
#     train_env = DummyVecEnv([make_train_env])
#     train_env = VecNormalize(
#         train_env, 
#         norm_obs=True, 
#         norm_reward=False,
#         clip_obs=10.0
#     )
    
#     # í‰ê°€ í™˜ê²½ ì„¤ì • (ì •ê·œí™” í¬í•¨)
#     eval_env = DummyVecEnv([make_eval_env])
#     eval_env = VecNormalize(
#         eval_env,
#         norm_obs=True,
#         norm_reward=False,
#         training=False,  # í‰ê°€ì‹œì—ëŠ” í†µê³„ ì—…ë°ì´íŠ¸ ì•ˆí•¨
#         clip_obs=10.0
#     )
    
#     # 3) ì½œë°± ì„¤ì •
#     # EvalCallback - TensorBoardì— eval ë©”íŠ¸ë¦­ ê¸°ë¡
#     eval_callback = EvalCallback(
#         eval_env,
#         best_model_save_path="runs/best_model",
#         log_path="runs/franka_reach",
#         eval_freq=50 * 2048,  # 50 iterationë§ˆë‹¤
#         n_eval_episodes=5,     # 5 ì—í”¼ì†Œë“œ í‰ê· 
#         deterministic=True,
#         verbose=1
#     )
    
#     # VideoRecorderCallback - ë¹„ë””ì˜¤ ë…¹í™”
#     video_callback = VideoRecorderCallback(
#         eval_env_fn=make_eval_env,
#         video_folder="videos",
#         video_length=100,
#         verbose=1
#     )
    
#     # ì½œë°± ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ê¸°
#     callbacks = CallbackList([eval_callback, video_callback])
    
#     # 4) PPO ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
#     model = PPO(
#         "MlpPolicy",
#         train_env,
#         learning_rate=3e-4,
#         n_steps=2048,
#         batch_size=64,
#         n_epochs=10,
#         gamma=0.99,
#         gae_lambda=0.95,
#         clip_range=0.2,
#         ent_coef=0.01,
#         vf_coef=0.5,
#         max_grad_norm=0.5,
#         verbose=1,
#         tensorboard_log="runs/franka_reach"
#     )
    
#     print("="*60)
#     print("PPO Training on Franka Reach Environment")
#     print(f"- Initial pose: {init_qpos if init_qpos else 'lap_start (from XML)'}")
#     print(f"- Goal pose: {goal_pose if goal_pose else 'lap_end (from XML)'}")
#     print(f"- Recording video every 50 iterations")
#     print(f"- Evaluating every 50 iterations (5 episodes)")
#     print(f"- Videos will be saved to: videos/")
#     print(f"- TensorBoard logs: runs/franka_reach")
#     print("="*60)
    
#     # ì •ê·œí™” í†µê³„ ë™ê¸°í™”
#     eval_env.obs_rms = train_env.obs_rms
#     eval_env.ret_rms = train_env.ret_rms
    
#     try:
#         model.learn(
#             total_timesteps=500_000,
#             callback=callbacks,
#             log_interval=10,
#             tb_log_name="PPO"  # TensorBoard ë¡œê·¸ ì´ë¦„
#         )
        
#         # ëª¨ë¸ ë° ì •ê·œí™” í†µê³„ ì €ì¥
#         model.save("ppo_franka_reach")
#         train_env.save("runs/vec_normalize.pkl")
        
#         print("\nâœ… Training completed successfully!")
#         print(f"- Model saved: ppo_franka_reach.zip")
#         print(f"- Normalization stats: runs/vec_normalize.pkl")
        
#         # ì €ì¥ëœ ë¹„ë””ì˜¤ ëª©ë¡ ì¶œë ¥
#         videos = sorted([f for f in os.listdir("videos") if f.endswith('.mp4')])
#         print(f"\nğŸ“¹ Saved videos ({len(videos)} total):")
#         for video in videos:
#             print(f"   - {video}")
        
#     except KeyboardInterrupt:
#         print("\nâš ï¸  Training interrupted by user")
#         model.save("ppo_franka_reach_interrupted")
        
#     except Exception as e:
#         print(f"\nâŒ Error during training: {e}")
#         raise
        
#     finally:
#         train_env.close()
#         eval_env.close()


#2025.06.08 13:02 - ì˜ìƒ í”½ì…€ ì´ìƒ
# import argparse
# import os
# import imageio
# import numpy as np
# from stable_baselines3 import PPO
# from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList
# from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
# from stable_baselines3.common.monitor import Monitor
# from mujoco_rl_env.env.franka_reach_env import FrankaReachEnv

# class VideoRecorderCallback(BaseCallback):
#     """50 iterationë§ˆë‹¤ ë¹„ë””ì˜¤ë¥¼ ì €ì¥í•˜ëŠ” ì»¤ìŠ¤í…€ ì½œë°±"""
#     def __init__(self, eval_env_fn, video_folder="videos", video_length=100, verbose=1):
#         super().__init__(verbose)
#         self.eval_env_fn = eval_env_fn
#         self.video_folder = video_folder
#         self.video_length = video_length
#         self.iteration_count = 0
#         os.makedirs(video_folder, exist_ok=True)
        
#     def _on_step(self) -> bool:
#         # ë§¤ 2048 ìŠ¤í…(1 iteration)ë§ˆë‹¤ ì¹´ìš´íŠ¸
#         if self.n_calls % 2048 == 0:
#             self.iteration_count += 1
            
#             # 50 iterationë§ˆë‹¤ ë¹„ë””ì˜¤ ë…¹í™”
#             if self.iteration_count % 50 == 0:
#                 self._record_video()
                
#         return True
    
#     def _record_video(self):
#         """í˜„ì¬ ì •ì±…ìœ¼ë¡œ ë¹„ë””ì˜¤ ë…¹í™”"""
#         if self.verbose > 0:
#             print(f"\nğŸ¬ Recording video at iteration {self.iteration_count}...")
        
#         # í‰ê°€ìš© í™˜ê²½ ìƒì„± (ì •ê·œí™” ì—†ì´)
#         env = self.eval_env_fn()
        
#         # VecNormalize ë˜í¼ê°€ ìˆë‹¤ë©´ ì •ê·œí™” í†µê³„ ë™ê¸°í™”
#         if hasattr(self.model, 'get_vec_normalize_env'):
#             vec_norm = self.model.get_vec_normalize_env()
#             if vec_norm is not None:
#                 eval_env_norm = VecNormalize(DummyVecEnv([lambda: env]), training=False)
#                 eval_env_norm.obs_rms = vec_norm.obs_rms
#                 eval_env_norm.ret_rms = vec_norm.ret_rms
#                 obs = eval_env_norm.reset()
#                 eval_env = eval_env_norm
#             else:
#                 obs = env.reset()
#         else:
#             obs = env.reset()
        
#         frames = []
#         episode_reward = 0
        
#         for step in range(self.video_length):
#             # í˜„ì¬ ì •ì±…ìœ¼ë¡œ í–‰ë™ ì„ íƒ
#             if isinstance(eval_env, VecNormalize):
#                 action, _ = self.model.predict(obs, deterministic=True)
#                 obs, reward, done, info = eval_env.step(action)
#                 # ì‹¤ì œ í™˜ê²½ì—ì„œ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
#                 frame = eval_env.venv.envs[0].render(mode="rgb_array")
#             else:
#                 action, _ = self.model.predict(obs, deterministic=True)
#                 obs, reward, done, info = env.step(action)
#                 frame = env.render(mode="rgb_array")
            
#             episode_reward += reward
            
#             if frame is not None:
#                 frames.append(frame)
            
#             if done:
#                 break
        
#         # ë¹„ë””ì˜¤ ì €ì¥
#         if frames:
#             start_iter = self.iteration_count - 49
#             end_iter = self.iteration_count
#             filename = f"{self.video_folder}/iteration_{start_iter:03d}-{end_iter:03d}.mp4"
            
#             imageio.mimsave(filename, frames, fps=30)
            
#             if self.verbose > 0:
#                 print(f"âœ… Video saved: {filename}")
#                 print(f"   Episode reward: {episode_reward:.2f}")
#                 print(f"   Episode length: {len(frames)}")
        
#         # í™˜ê²½ ì •ë¦¬
#         if isinstance(eval_env, VecNormalize):
#             eval_env.close()
#         else:
#             env.close()

# if __name__ == "__main__":
#     parser = argparse.ArgumentParser(
#         description="Train PPO on the Franka reach environment"
#     )
#     parser.add_argument(
#         "--xml", default="models/fr3_reach.xml",
#         help="Path to your MuJoCo XML model"
#     )
#     parser.add_argument(
#         "--init", nargs='*', type=float,
#         default=None,  # Noneìœ¼ë¡œ ê¸°ë³¸ê°’ ì„¤ì •
#         help="Initial joint positions (optional)"
#     )
#     parser.add_argument(
#         "--goal", nargs=7, type=float,
#         default=None,  # Noneìœ¼ë¡œ ê¸°ë³¸ê°’ ì„¤ì •
#         help="Goal pose: x y z qx qy qz qw"
#     )
#     args = parser.parse_args()

#     # ë””ë ‰í† ë¦¬ ìƒì„±
#     os.makedirs("videos", exist_ok=True)
#     os.makedirs("runs/best_model", exist_ok=True)
#     os.makedirs("runs/franka_reach", exist_ok=True)

#     # init ì¸ì ì²˜ë¦¬
#     if args.init is not None and len(args.init) > 0:
#         init_qpos = args.init
#     else:
#         init_qpos = None  # í™˜ê²½ì—ì„œ ê¸°ë³¸ê°’ ì‚¬ìš©
#         print("â„¹ï¸  No initial pose provided, using lap_start from XML")

#     # goal ì¸ì ì²˜ë¦¬
#     if args.goal is not None:
#         goal_pose = args.goal
#     else:
#         goal_pose = None  # í™˜ê²½ì—ì„œ lap_end ì‚¬ìš©
#         print("â„¹ï¸  No goal pose provided, using lap_end from XML")

#     # 1) í•™ìŠµìš© í™˜ê²½ ìƒì„± (Monitorë¡œ ê°ì‹¸ê¸°)
#     def make_train_env():
#         env = FrankaReachEnv(
#             xml_path=args.xml,
#             init_qpos=init_qpos,
#             goal_pose=goal_pose,
#             render_mode=None
#         )
#         # Monitorë¡œ ê°ì‹¸ì„œ ì—í”¼ì†Œë“œ í†µê³„ ê¸°ë¡
#         env = Monitor(env)
#         return env
    
#     # 2) í‰ê°€ìš© í™˜ê²½ ìƒì„± í•¨ìˆ˜
#     def make_eval_env():
#         env = FrankaReachEnv(
#             xml_path=args.xml,
#             init_qpos=init_qpos,
#             goal_pose=goal_pose,
#             render_mode="rgb_array"
#         )
#         # í‰ê°€ í™˜ê²½ë„ Monitorë¡œ ê°ì‹¸ê¸°
#         env = Monitor(env)
#         return env
    
#     # í•™ìŠµ í™˜ê²½ ì„¤ì •
#     train_env = DummyVecEnv([make_train_env])
#     train_env = VecNormalize(
#         train_env, 
#         norm_obs=True, 
#         norm_reward=False,
#         clip_obs=10.0
#     )
    
#     # í‰ê°€ í™˜ê²½ ì„¤ì • (ì •ê·œí™” í¬í•¨)
#     eval_env = DummyVecEnv([make_eval_env])
#     eval_env = VecNormalize(
#         eval_env,
#         norm_obs=True,
#         norm_reward=False,
#         training=False,  # í‰ê°€ì‹œì—ëŠ” í†µê³„ ì—…ë°ì´íŠ¸ ì•ˆí•¨
#         clip_obs=10.0
#     )
    
#     # 3) ì½œë°± ì„¤ì •
#     # EvalCallback - TensorBoardì— eval ë©”íŠ¸ë¦­ ê¸°ë¡
#     eval_callback = EvalCallback(
#         eval_env,
#         best_model_save_path="runs/best_model",
#         log_path="runs/franka_reach",
#         eval_freq=50 * 2048,  # 50 iterationë§ˆë‹¤
#         n_eval_episodes=5,     # 5 ì—í”¼ì†Œë“œ í‰ê· 
#         deterministic=True,
#         verbose=1
#     )
    
#     # VideoRecorderCallback - ë¹„ë””ì˜¤ ë…¹í™”
#     video_callback = VideoRecorderCallback(
#         eval_env_fn=make_eval_env,
#         video_folder="videos",
#         video_length=100,
#         verbose=1
#     )
    
#     # ì½œë°± ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ê¸°
#     callbacks = CallbackList([eval_callback, video_callback])
    
#     # 4) PPO ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
#     model = PPO(
#         "MlpPolicy",
#         train_env,
#         learning_rate=3e-4,
#         n_steps=2048,
#         batch_size=64,
#         n_epochs=10,
#         gamma=0.99,
#         gae_lambda=0.95,
#         clip_range=0.2,
#         ent_coef=0.01,
#         vf_coef=0.5,
#         max_grad_norm=0.5,
#         verbose=1,
#         tensorboard_log="runs/franka_reach"
#     )
    
#     print("="*60)
#     print("PPO Training on Franka Reach Environment")
#     print(f"- Initial pose: {init_qpos if init_qpos else 'lap_start (from XML)'}")
#     print(f"- Goal pose: {goal_pose if goal_pose else 'lap_end (from XML)'}")
#     print(f"- Recording video every 50 iterations")
#     print(f"- Evaluating every 50 iterations (5 episodes)")
#     print(f"- Videos will be saved to: videos/")
#     print(f"- TensorBoard logs: runs/franka_reach")
#     print("="*60)
    
#     # ì •ê·œí™” í†µê³„ ë™ê¸°í™”
#     eval_env.obs_rms = train_env.obs_rms
#     eval_env.ret_rms = train_env.ret_rms
    
#     try:
#         model.learn(
#             total_timesteps=500_000,
#             callback=callbacks,
#             log_interval=10,
#             tb_log_name="PPO"  # TensorBoard ë¡œê·¸ ì´ë¦„
#         )
        
#         # ëª¨ë¸ ë° ì •ê·œí™” í†µê³„ ì €ì¥
#         model.save("ppo_franka_reach")
#         train_env.save("runs/vec_normalize.pkl")
        
#         print("\nâœ… Training completed successfully!")
#         print(f"- Model saved: ppo_franka_reach.zip")
#         print(f"- Normalization stats: runs/vec_normalize.pkl")
        
#         # ì €ì¥ëœ ë¹„ë””ì˜¤ ëª©ë¡ ì¶œë ¥
#         videos = sorted([f for f in os.listdir("videos") if f.endswith('.mp4')])
#         print(f"\nğŸ“¹ Saved videos ({len(videos)} total):")
#         for video in videos:
#             print(f"   - {video}")
        
#     except KeyboardInterrupt:
#         print("\nâš ï¸  Training interrupted by user")
#         model.save("ppo_franka_reach_interrupted")
        
#     except Exception as e:
#         print(f"\nâŒ Error during training: {e}")
#         raise
        
#     finally:
#         train_env.close()
#         eval_env.close()



# 2025.06.08 12:42 - ë¹„ë””ì˜¤ ê¹¨ì§ í˜„ìƒ 
# import argparse
# import os
# import imageio
# import numpy as np
# from stable_baselines3 import PPO
# from stable_baselines3.common.callbacks import BaseCallback
# from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
# from mujoco_rl_env.env.franka_reach_env import FrankaReachEnv

# class VideoRecorderCallback(BaseCallback):
#     """50 iterationë§ˆë‹¤ ë¹„ë””ì˜¤ë¥¼ ì €ì¥í•˜ëŠ” ì»¤ìŠ¤í…€ ì½œë°±"""
#     def __init__(self, eval_env_fn, video_folder="videos", video_length=100, verbose=1):
#         super().__init__(verbose)
#         self.eval_env_fn = eval_env_fn
#         self.video_folder = video_folder
#         self.video_length = video_length
#         self.iteration_count = 0
#         os.makedirs(video_folder, exist_ok=True)
        
#     def _on_step(self) -> bool:
#         # ë§¤ 2048 ìŠ¤í…(1 iteration)ë§ˆë‹¤ ì¹´ìš´íŠ¸
#         if self.n_calls % 2048 == 0:
#             self.iteration_count += 1
            
#             # 50 iterationë§ˆë‹¤ ë¹„ë””ì˜¤ ë…¹í™”
#             if self.iteration_count % 50 == 0:
#                 self._record_video()
                
#         return True
    
#     def _record_video(self):
#         """í˜„ì¬ ì •ì±…ìœ¼ë¡œ ë¹„ë””ì˜¤ ë…¹í™”"""
#         if self.verbose > 0:
#             print(f"\nğŸ¬ Recording video at iteration {self.iteration_count}...")
        
#         # í‰ê°€ìš© í™˜ê²½ ìƒì„±
#         env = self.eval_env_fn()
#         obs = env.reset()
        
#         frames = []
#         episode_reward = 0
        
#         for step in range(self.video_length):
#             # í˜„ì¬ ì •ì±…ìœ¼ë¡œ í–‰ë™ ì„ íƒ
#             action, _ = self.model.predict(obs, deterministic=True)
#             obs, reward, done, info = env.step(action)
#             episode_reward += reward
            
#             # í”„ë ˆì„ ìº¡ì²˜
#             frame = env.render(mode="rgb_array")
#             if frame is not None:
#                 frames.append(frame)
            
#             if done:
#                 break
        
#         # ë¹„ë””ì˜¤ ì €ì¥
#         if frames:
#             start_iter = self.iteration_count - 49
#             end_iter = self.iteration_count
#             filename = f"{self.video_folder}/iteration_{start_iter:03d}-{end_iter:03d}.mp4"
            
#             imageio.mimsave(filename, frames, fps=30)
            
#             if self.verbose > 0:
#                 print(f"âœ… Video saved: {filename}")
#                 print(f"   Episode reward: {episode_reward:.2f}")
#                 print(f"   Episode length: {len(frames)}")
        
#         env.close()

# if __name__ == "__main__":
#     parser = argparse.ArgumentParser(
#         description="Train PPO on the Franka reach environment"
#     )
#     parser.add_argument(
#         "--xml", default="models/fr3_reach.xml",
#         help="Path to your MuJoCo XML model"
#     )
#     parser.add_argument(
#         "--init", nargs='*', type=float,
#         default=None,  # Noneìœ¼ë¡œ ê¸°ë³¸ê°’ ì„¤ì •
#         help="Initial joint positions (optional)"
#     )
#     parser.add_argument(
#         "--goal", nargs=7, type=float,
#         help="Goal pose: x y z qx qy qz qw"
#     )
#     args = parser.parse_args()

#     # ë””ë ‰í† ë¦¬ ìƒì„±
#     os.makedirs("videos", exist_ok=True)
#     os.makedirs("runs/best_model", exist_ok=True)
#     os.makedirs("runs/franka_reach", exist_ok=True)

#     # init ì¸ì ì²˜ë¦¬
#     if args.init is not None and len(args.init) > 0:
#         init_qpos = args.init
#     else:
#         init_qpos = None  # í™˜ê²½ì—ì„œ ê¸°ë³¸ê°’ ì‚¬ìš©
#         print("â„¹ï¸  No initial pose provided, using environment defaults")

#     # 1) í•™ìŠµìš© í™˜ê²½ ìƒì„±
#     def make_train_env():
#         return FrankaReachEnv(
#             xml_path=args.xml,
#             init_qpos=init_qpos,
#             goal_pose=args.goal,
#             render_mode=None
#         )
    
#     # 2) í‰ê°€ìš© í™˜ê²½ ìƒì„± í•¨ìˆ˜
#     def make_eval_env():
#         env = FrankaReachEnv(
#             xml_path=args.xml,
#             init_qpos=init_qpos,
#             goal_pose=args.goal,
#             render_mode="rgb_array"
#         )
#         return env
    
#     # í•™ìŠµ í™˜ê²½ ì„¤ì •
#     train_env = DummyVecEnv([make_train_env])
#     train_env = VecNormalize(
#         train_env, 
#         norm_obs=True, 
#         norm_reward=False,
#         clip_obs=10.0
#     )
    
#     # 3) ë¹„ë””ì˜¤ ë…¹í™” ì½œë°±
#     video_callback = VideoRecorderCallback(
#         eval_env_fn=make_eval_env,
#         video_folder="videos",
#         video_length=100,
#         verbose=1
#     )
    
#     # 4) PPO ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
#     model = PPO(
#         "MlpPolicy",
#         train_env,
#         learning_rate=3e-4,
#         n_steps=2048,
#         batch_size=64,
#         n_epochs=10,
#         gamma=0.99,
#         gae_lambda=0.95,
#         clip_range=0.2,
#         ent_coef=0.01,
#         vf_coef=0.5,
#         max_grad_norm=0.5,
#         verbose=1,
#         tensorboard_log="runs/franka_reach"
#     )
    
#     print("="*60)
#     print("PPO Training on Franka Reach Environment")
#     print(f"- Initial pose: {init_qpos if init_qpos else 'Default (zeros)'}")
#     print(f"- Goal pose: {args.goal}")
#     print(f"- Recording video every 50 iterations")
#     print(f"- Videos will be saved to: videos/")
#     print("="*60)
    
#     try:
#         model.learn(
#             total_timesteps=500_000,
#             callback=video_callback,
#             log_interval=10
#         )
        
#         # ëª¨ë¸ ë° ì •ê·œí™” í†µê³„ ì €ì¥
#         model.save("ppo_franka_reach")
#         train_env.save("runs/vec_normalize.pkl")
        
#         print("\nâœ… Training completed successfully!")
#         print(f"- Model saved: ppo_franka_reach.zip")
#         print(f"- Normalization stats: runs/vec_normalize.pkl")
        
#         # ì €ì¥ëœ ë¹„ë””ì˜¤ ëª©ë¡ ì¶œë ¥
#         videos = sorted([f for f in os.listdir("videos") if f.endswith('.mp4')])
#         print(f"\nğŸ“¹ Saved videos ({len(videos)} total):")
#         for video in videos:
#             print(f"   - {video}")
        
#     except KeyboardInterrupt:
#         print("\nâš ï¸  Training interrupted by user")
#         model.save("ppo_franka_reach_interrupted")
        
#     except Exception as e:
#         print(f"\nâŒ Error during training: {e}")
#         raise
        
#     finally:
#         train_env.close()

'''2025.06.07 23:49 ì´ì „ ì‘ì—… - í´ë¡œë“œ ì „
#/home/minjun/rl_ws/src/mujoco_rl_env/train.py
import argparse
import os
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize #normalized
from stable_baselines3.common.vec_env import VecVideoRecorder #videoì €ì¥
from mujoco_rl_env.env.franka_reach_env import FrankaReachEnv

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Train PPO on the Franka reach environment"
    )
    parser.add_argument(
        "--xml", default="models/fr3_reach.xml",
        help="Path to your MuJoCo XML model"
    )
    parser.add_argument(
        "--init", nargs=14, type=float,
        default=[0]*14,
        help="Initial joint positions (14 values)"
    )
    parser.add_argument(
        "--goal", nargs=7, type=float,
        help="Goal pose: x y z qx qy qz qw"
    )
    args = parser.parse_args()

    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("videos", exist_ok=True)
    os.makedirs("runs/best_model", exist_ok=True)
    os.makedirs("runs/franka_reach", exist_ok=True)

    # 1) í•™ìŠµìš© í™˜ê²½ ìƒì„± (render_mode ì—†ìŒ)
    def make_train_env():
        return FrankaReachEnv(
            xml_path=args.xml,
            init_qpos=args.init,  # ì „ì²´ 14ê°œ ê°’ ëª¨ë‘ ì „ë‹¬
            goal_pose=args.goal,
            render_mode=None  # í•™ìŠµ ì¤‘ì—ëŠ” ë Œë”ë§ ë¶ˆí•„ìš”
        )
    
    train_env = DummyVecEnv([make_train_env])
    
    # 2) í‰ê°€/ë…¹í™”ìš© í™˜ê²½ ìƒì„± - ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„!
    def make_eval_env():
        env = FrankaReachEnv(
            xml_path=args.xml,
            init_qpos=args.init,  # ì „ì²´ 14ê°œ ê°’ ëª¨ë‘ ì „ë‹¬
            goal_pose=args.goal,
            render_mode="rgb_array"  # ë¹„ë””ì˜¤ ë…¹í™”ë¥¼ ìœ„í•´ í•„ìˆ˜
        )
        # í™˜ê²½ì´ render_mode ì†ì„±ì„ ê°–ë„ë¡ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
        env.render_mode = "rgb_array"
        return env
    
    eval_env = DummyVecEnv([make_eval_env])
    
    # í•µì‹¬ ìˆ˜ì •: DummyVecEnvì— render_mode ì†ì„± ìˆ˜ë™ ì¶”ê°€!!!
    eval_env.render_mode = "rgb_array"
    
    # 3) ë¹„ë””ì˜¤ ë…¹í™” ë˜í¼ ì ìš©
    record_freq = 50 * 2048  # 50 iterationë§ˆë‹¤
    video_eval_env = VecVideoRecorder(
        eval_env,
        video_folder="videos/",
        record_video_trigger=lambda x: x % record_freq == 0,
        video_length=100,  # ì—í”¼ì†Œë“œ ê¸¸ì´
        name_prefix="franka_reach"
    )
    
    # 4) ì •ê·œí™” ë˜í¼ (í•™ìŠµ í™˜ê²½ë§Œ)
    train_env = VecNormalize(
        train_env, 
        norm_obs=True, 
        norm_reward=False,
        clip_obs=10.0
    )
    
    # í‰ê°€ í™˜ê²½ë„ ì •ê·œí™” (training=False)
    eval_env_normalized = VecNormalize(
        video_eval_env,  # ë¹„ë””ì˜¤ ë˜í¼ ìœ„ì— ì •ê·œí™” ë˜í¼
        norm_obs=True,
        norm_reward=False,
        training=False,
        clip_obs=10.0
    )
    
    # 5) í‰ê°€ ì½œë°± ì„¤ì •
    eval_callback = EvalCallback(
        eval_env_normalized,
        best_model_save_path="runs/best_model",
        log_path="runs/franka_reach",
        eval_freq=record_freq,  # 50 iterationë§ˆë‹¤
        n_eval_episodes=1,      # í‰ê°€ ì‹œ 1 ì—í”¼ì†Œë“œë§Œ
        deterministic=True,
        verbose=1
    )
    
    # 6) PPO ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
    model = PPO(
        "MlpPolicy",
        train_env,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,
        vf_coef=0.5,
        max_grad_norm=0.5,
        verbose=1,
        tensorboard_log="runs/franka_reach"
    )
    
    print("="*60)
    print("PPO Training on Franka Reach Environment")
    print(f"- Training with {train_env.num_envs} environment(s)")
    print(f"- Recording video every {record_freq} steps (50 iterations)")
    print(f"- Videos will be saved to: videos/")
    print(f"- Tensorboard logs: runs/franka_reach")
    print("="*60)
    
    try:
        model.learn(
            total_timesteps=500_000,
            callback=eval_callback,
            log_interval=10
        )
        
        # ëª¨ë¸ ë° ì •ê·œí™” í†µê³„ ì €ì¥
        model.save("ppo_franka_reach")
        train_env.save("runs/vec_normalize.pkl")
        
        print("\nâœ… Training completed successfully!")
        print(f"- Model saved: ppo_franka_reach.zip")
        print(f"- Normalization stats: runs/vec_normalize.pkl")
        print(f"- Videos saved in: videos/")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  Training interrupted by user")
        model.save("ppo_franka_reach_interrupted")
        
    except Exception as e:
        print(f"\nâŒ Error during training: {e}")
        raise
        
    finally:
        # í™˜ê²½ ì •ë¦¬
        train_env.close()
        eval_env_normalized.close()
'''
# # shapr 7ì¸ì§€ 14ì¸ì§€ì— ëŒ€í•œ ë¬¸ì œ
# if __name__ == "__main__":
#     parser = argparse.ArgumentParser(
#         description="Train PPO on the Franka reach environment"
#     )
#     parser.add_argument(
#         "--xml", default="models/fr3_reach.xml",
#         help="Path to your MuJoCo XML model"
#     )
#     parser.add_argument(
#         "--init", nargs=14, type=float,
#         default=[0]*14,  # 14ê°œë¡œ ìˆ˜ì •
#         help="Initial joint positions (14 values)"
#     )
#     parser.add_argument(
#         "--goal", nargs=7, type=float,
#         help="Goal pose: x y z qx qy qz qw"
#     )
#     args = parser.parse_args()

#     # ë””ë ‰í† ë¦¬ ìƒì„±
#     os.makedirs("videos", exist_ok=True)
#     os.makedirs("runs/best_model", exist_ok=True)
#     os.makedirs("runs/franka_reach", exist_ok=True)

#     # 1) í•™ìŠµìš© í™˜ê²½ ìƒì„±
#     def make_train_env():
#         return FrankaReachEnv(
#             xml_path=args.xml,
#             init_qpos=args.init[:7],  # ì²˜ìŒ 7ê°œë§Œ ì‚¬ìš©
#             goal_pose=args.goal,
#             render_mode=None
#         )
    
#     # 2) í‰ê°€ìš© í™˜ê²½ ìƒì„± (ë¹„ë””ì˜¤ ë…¹í™”ìš©)
#     def make_eval_env():
#         return FrankaReachEnv(
#             xml_path=args.xml,
#             init_qpos=args.init[:7],  # ì²˜ìŒ 7ê°œë§Œ ì‚¬ìš©
#             goal_pose=args.goal,
#             render_mode="rgb_array"
#         )
    
#     # í™˜ê²½ ë˜í•‘
#     train_env = DummyVecEnv([make_train_env])
#     eval_env = DummyVecEnv([make_eval_env])
    
#     # ì¤‘ìš”: DummyVecEnvì— render_mode ì†ì„± ì¶”ê°€
#     eval_env.render_mode = "rgb_array"
    
#     # 3) ë¹„ë””ì˜¤ ë…¹í™” ë˜í¼
#     record_freq = 50 * 2048  # 50 iterationë§ˆë‹¤
#     video_eval_env = VecVideoRecorder(
#         eval_env,
#         video_folder="videos/",
#         record_video_trigger=lambda x: x % record_freq == 0,
#         video_length=100,  # ìµœëŒ€ 100 ìŠ¤í…
#         name_prefix="franka_reach"
#     )
    
#     # 4) ì •ê·œí™” ë˜í¼
#     train_env = VecNormalize(
#         train_env, 
#         norm_obs=True, 
#         norm_reward=False,
#         clip_obs=10.0
#     )
    
#     eval_env_normalized = VecNormalize(
#         video_eval_env,
#         norm_obs=True,
#         norm_reward=False,
#         training=False,
#         clip_obs=10.0
#     )
    
#     # 5) í‰ê°€ ì½œë°±
#     eval_callback = EvalCallback(
#         eval_env_normalized,
#         best_model_save_path="runs/best_model",
#         log_path="runs/franka_reach",
#         eval_freq=record_freq,
#         n_eval_episodes=1,
#         deterministic=True,
#         verbose=1
#     )
    
#     # 6) PPO ëª¨ë¸ ìƒì„±
#     model = PPO(
#         "MlpPolicy",
#         train_env,
#         learning_rate=3e-4,
#         n_steps=2048,
#         batch_size=64,
#         n_epochs=10,
#         gamma=0.99,
#         gae_lambda=0.95,
#         clip_range=0.2,
#         ent_coef=0.01,
#         vf_coef=0.5,
#         max_grad_norm=0.5,
#         verbose=1,
#         tensorboard_log="runs/franka_reach",
#         device="auto"  # GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ ìë™ ì„ íƒ
#     )
    
#     print("="*50)
#     print("Training PPO on Franka Reach Environment")
#     print(f"Recording videos every {record_freq} steps (50 iterations)")
#     print(f"Videos will be saved to: videos/")
#     print("="*50)
    
#     try:
#         # í•™ìŠµ ì‹œì‘
#         model.learn(
#             total_timesteps=500_000,
#             callback=eval_callback,
#             log_interval=10
#         )
        
#         # ëª¨ë¸ ì €ì¥
#         model.save("ppo_franka_reach")
#         train_env.save("runs/vec_normalize.pkl")
        
#         print("\nTraining completed successfully!")
#         print(f"Model saved as: ppo_franka_reach.zip")
#         print(f"Normalization stats saved as: runs/vec_normalize.pkl")
        
#     except Exception as e:
#         print(f"\nError during training: {e}")
#         raise
    
#     finally:
#         # í™˜ê²½ ì •ë¦¬
#         train_env.close()
#         eval_env_normalized.close()



# class VideoRecordCallback(EvalCallback):
#     """ 50 iteration ë§ˆë‹¤ ë¹„ë””ì˜¤ ë…¹í™”í•˜ëŠ” ì»¤ìŠ¤í…€ ì½œë°± """
#     def __init__(self, *args, **kwargs):
#         super().__init__(*args, **kwargs)
#         self.iteration_count = 0
        
#     def _on_step(self) -> bool:
#         result = super()._on_step()

#         if self.n_calls % self.model.n_steps == 0:
#             self.iteration_count += 1
        
#         return result

# if __name__ == "__main__":
#     parser = argparse.ArgumentParser(
#         description="Train PPO on the Franka reach environment"
#     )
#     parser.add_argument(
#         "--xml", default="models/fr3_reach.xml",
#         help="Path to your MuJoCo XML model"
#     )
#     parser.add_argument(
#         "--init", nargs=14, type=float,
#         default=[0]*7,
#         help="Initial joint positions (7 values)"
#     )
#     parser.add_argument(
#         "--goal", nargs=7, type=float,
#         help="Goal pose: x y z qx qy qz qw"
#     )
#     args = parser.parse_args()

#     os.makedirs("videos", exist_ok=True)
#     os.makedirs("runs/best_model", exist_ok=True)
#     os.makedirs("runs/franka_reach", exist_ok=True)
    
#     # env = FrankaReachEnv(
#     #     xml_path=args.xml,
#     #     init_qpos=args.init,
#     #     goal_pose=args.goal
#     # )
#     # ê¸°ì¡´ì—” model = PPO~ë¶€í„°ë§Œ ìˆì—ˆìŒ
#     # 1) í•™ìŠµìš©/í‰ê°€ìš© env ë˜í•‘
#     '''
#     # DummyVecEnv: ë²¡í„°í™”ëœ í™˜ê²½ ë˜í¼ -> VecNormalizeë¡œ ê´€ì¸¡/ë³´ìƒ ì •ê·œí™” -> VecVideoRecorderë¡œ ë¹„ë””ì˜¤ ë…¹í™”
#     train_env = DummyVecEnv([lambda: env])
#     eval_env  = DummyVecEnv([lambda: FrankaReachEnv(
#         xml_path=args.xml,
#         init_qpos=args.init,
#         goal_pose=args.goal,  # ê³ ì • goalìœ¼ë¡œ í‰ê°€
#         render_mode="rgb_array"   # â† ì´ê±¸ ë°˜ë“œì‹œ ì§€ì •í•´ì•¼ ë¹„ë””ì˜¤ ë…¹í™” ê°€ëŠ¥
#     )])

#     #2) ê´€ì¸¡ë§Œ ì •ê·œí™” (í•™ìŠµ ë³´ìƒì€ ê·¸ëŒ€ë¡œ)
#     train_env = VecNormalize(train_env, norm_obs=True, norm_reward=False)
#     eval_env  = VecNormalize(eval_env,  norm_obs=True, norm_reward=False,
#                               training=False)
#     # 2.1) ë¹„ë””ì˜¤ ë…¹í™”: 50 iteration ë§ˆë‹¤ í•œ ë²ˆì”©, í•œ ì—í”¼ì†Œë“œ ê¸¸ì´ ë§Œí¼ ì €ì¥
#     #   step: ì „ì²´ timesteps, so record when step % (2048*50) == 0
#     record_freq = 2048 * 50
#     video_folder = "videos/"
#     eval_env = VecVideoRecorder(
#         eval_env,
#         video_folder=video_folder,
#         record_video_trigger=lambda step: step % record_freq == 0,
#         video_length=env.max_steps,   # í•œ ì—í”¼ì†Œë“œ ê¸¸ì´(100 steps)ë§Œí¼ ë…¹í™”
#         name_prefix="franka_reach"
#     )    
#     '''
#     # video ë…¹í™” ë¨¼ì €
#     # â†’ 1) DummyVecEnv â†’ 2) VecVideoRecorder â†’ 3) VecNormalize

#     # í•™ìŠµìš© í™˜ê²½ (ë Œë”ë§ ì—†ìŒ)
#     train_env = DummyVecEnv([lambda: FrankaReachEnv(
#         xml_path=args.xml,
#         init_qpos=args.init,
#         goal_pose=args.goal,
#         render_mode=None  # í•™ìŠµìš© envëŠ” ë Œë”ë§ ì•ˆí•¨
#     )])

#     # í‰ê°€ìš© í™˜ê²½ (ë Œë”ë§ ìˆìŒ)
#     eval_env = DummyVecEnv([lambda: FrankaReachEnv(
#         xml_path=args.xml,
#         init_qpos=args.init,
#         goal_pose=args.goal,  # ê³ ì • goalìœ¼ë¡œ í‰ê°€
#         render_mode="rgb_array"  # ë¹„ë””ì˜¤ ë…¹í™”ë¥¼ ìœ„í•´ ë Œë”ë§ ëª¨ë“œ ì§€ì •
#     )])


#     # raw_eval_env = DummyVecEnv([lambda: FrankaReachEnv(
#     #     xml_path=args.xml,
#     #     init_qpos=args.init,
#     #     goal_pose=args.goal,
#     #     render_mode="rgb_array"       # â† ì—¬ê¸°ì„œ ë¯¸ë¦¬ ì§€ì •
#     # )])

#     # DummyVecEnv(â€¦) ìì²´ì—ë„ render_mode ë¶€ì°©!
#     #raw_eval_env.render_mode = "rgb_array"
#     # (2) ë¹„ë””ì˜¤ ë…¹í™” ë˜í¼ ë¨¼ì €
#     # 2.1) ë¹„ë””ì˜¤ ë…¹í™”: 50 iteration ë§ˆë‹¤ í•œ ë²ˆì”©, í•œ ì—í”¼ì†Œë“œ ê¸¸ì´ ë§Œí¼ ì €ì¥
#     #   step: ì „ì²´ timesteps, so record when step % (2048*50) == 0
#     record_freq = 2048 * 50
#     video_folder = "videos/"
#     video_eval_env = VecVideoRecorder(
#         eval_env,
#         video_folder=video_folder,
#         record_video_trigger=lambda step: step % record_freq == 0,
#         video_length=env.max_steps, #100
#         name_prefix="franka_reach"
#     )
#     # (3) ê·¸ë¦¬ê³  ë‚˜ì„œ ì •ê·œí™” ë˜í¼ (í•™ìŠµí™˜ê²½ë§Œ)
#     train_env = VecNormalize(train_env, norm_obs=True, norm_reward=False)
    
#     #í‰ê°€ í™˜ê²½ë„ ì •ê·œí™” (í•™ìŠµí•˜ì§€ ì•ŠìŒ)
#     eval_env_normalized  = VecNormalize(
#         video_eval_env,
#         norm_obs=True,
#         norm_reward=False,
#         training=False
#     )

    
#     # 3) EvalCallback: 50 iters(â‰ˆ50*2048 steps)ë§ˆë‹¤ í‰ê°€ ê¸°ë¡
#     eval_callback = VideoRecordCallback(
#         eval_env_normalized,
#         best_model_save_path="runs/best_model",
#         log_path="runs/franka_reach",
#         eval_freq=record_freq,
#         n_eval_episodes=1,
#         deterministic=True,
#         verbose = 1
#     )

#     # eval_callback = EvalCallback(
#     #     eval_env,
#     #     best_model_save_path="runs/best_model",
#     #     log_path="runs/franka_reach",
#     #     eval_freq=2048*50,
#     #     n_eval_episodes=5,
#     #     deterministic=True,
#     # )
#     # ==== ë³€ê²½ëœ ë¶€ë¶„ ë ====
#     # ê¸°ì¡´ì—” train_envëŒ€ì‹  envë§Œ ì‚¬ìš©í–ˆìŒ
#     # ì´í›„ model.learn(total_timesteps=500_000) ë¶€ë¶„ê¹Œì§€ ë™ì¼ , callbackì€ì—†ì—ˆìŒ
#     # ê·¸ëƒ¥ model = PPO("MlpPolicy", env, verbose=1)ë¡œë§Œ ì‚¬ìš©í–ˆìœ¼ë©´ -> hyperparameter ê¸°ë³¸ ì„¸íŒ…
#     # learning_rate	3Ã—10â»â´ n_steps	2048 batch_size	64 _epochs	10 gamma	0.99 gae_lambda	0.95 clip_range	0.2 ent_coef	0.0 vf_coef	0.5 max_grad_norm	0.5
#     '''
#     model = PPO(
#         "MlpPolicy", train_env,
#         verbose=1,
#         tensorboard_log="runs/franka_reach"
#     )'''

#     # 4) PPO ëª¨ë¸ ìƒì„±
#     model = PPO(
#         "MlpPolicy", train_env,``
#         learning_rate=3e-5,
#         n_steps=2048,
#         batch_size=128,
#         n_epochs=15,
#         gamma=0.99,
#         gae_lambda=0.95,
#         clip_range=0.2,
#         ent_coef=0.001,
#         vf_coef=0.5,
#         max_grad_norm=0.5,
#         verbose=1,
#         tensorboard_log="runs/franka_reach"
#     )

#     print("Starting training with video recording every 50 iterations...")
#     model.learn(
#         total_timesteps=500_000,
#         callback=eval_callback
#     )

#     # ëª¨ë¸ ì €ì¥
#     model.save("ppo_franka_reach")
    
#     # ì •ê·œí™” í†µê³„ ì €ì¥
#     train_env.save("runs/vec_normalize.pkl")
    
#     print("Training completed!")
#     print("Videos saved in: videos/")
#     print("Model saved as: ppo_franka_reach.zip")