
# ì˜ìƒ ìµœëŒ€ê¸¸ì´ 4ì´ˆ, ì´ˆê¸°ê°’ 0ìœ¼ë¡œ ì„¤ì •, xmlíŒŒì¼ì—ì„œ lap_startí‚¤í”„ë ˆì„ jointê°’ ì•ˆì„¤ì •ë˜ì–´ìˆìŒ.
# rolloutì•ˆë³´ì„, ì—í”¼ì†Œë“œ ì œëŒ€ë¡œ ì¢…ë£Œ x, videoì´ë¦„, íŒŒì¼ ê´€ë¦¬ ì•ˆë¨ (ë®ì–´ì“°ê¸°)
# ë¬¸ì œ ë‹¤ í•´ê²°
# /home/minjun/rl_ws/src/mujoco_rl_env/env/franka_reach_env.py
import gymnasium as gym  # gym â†’ gymnasiumìœ¼ë¡œ ë³€ê²½!
from gymnasium import spaces
import numpy as np
import mujoco

class FrankaReachEnv(gym.Env):
    metadata = {"render_modes": ["human", "rgb_array"]}
    """
    Gym environment for moving a Franka Panda arm in MuJoCo
    from an initial joint configuration to a goal pose, with a reward
    that encourages smooth, accurate motion.
    """
    def __init__(self, xml_path, init_qpos=None, goal_qpos=None, render_mode=None):
        super().__init__()
        # Load model and data
        self.model = mujoco.MjModel.from_xml_path(xml_path)
        self.data = mujoco.MjData(self.model)

        # Progress reward ê³„ìˆ˜
        self.kappa = 5.0

        # reset() ì‹œ ì„¸íŒ…ë  ê°’ë“¤
        self.init_pos_err = None
        self.prev_pos_err = None

        # # --- lap_start, lap_end site ID ì¡°íšŒ ---
        # self.start_site_id = mujoco.mj_name2id(
        #     self.model, mujoco.mjtObj.mjOBJ_SITE, "lap_start"
        # )
        # self.end_site_id = mujoco.mj_name2id(
        #     self.model, mujoco.mjtObj.mjOBJ_SITE, "lap_end"
        # )

        # render_modeì„¤ì •!
        self.render_mode = render_mode

        # Initial joint positions
        if init_qpos is None:
            # default to zeros if no keyframes
            self.init_qpos = np.zeros(self.model.nq)
        else:
            # ì…ë ¥ë°›ì€ init_qposê°€ nqë³´ë‹¤ ì‘ìœ¼ë©´ 0ìœ¼ë¡œ íŒ¨ë”©
            if len(init_qpos) < self.model.nq:
                self.init_qpos = np.zeros(self.model.nq)
                self.init_qpos[:len(init_qpos)] = init_qpos
            else:
                self.init_qpos = np.array(init_qpos[:self.model.nq])

        # Fixed goal pose [x, y, z, qx, qy, qz, qw]
        # goal pose -> goal jointstateë¡œ ë³€ê²½
        # self.goal_pose = np.array(goal_pose) if goal_pose is not None else None
        # Identify end-effector site (must exist in XML as <site name="ee_site" ...>)
        # self.ee_site_id = mujoco.mj_name2id(
        #     self.model, mujoco.mjtObj.mjOBJ_SITE, "ee_site"
        # )

        if goal_qpos is None:
            # sample once at init
            low = self.model.jnt_range[:, 0].astype(np.float32)
            high = self.model.jnt_range[:, 1].astype(np.float32)
            self.goal_qpos = np.random.uniform(low, high)
        else:
            arr_g = np.array(goal_qpos, dtype=np.float32)
            self.goal_qpos = arr_g[:self.model.nq] if arr_g.size >= self.model.nq else np.pad(arr_g, (0, self.model.nq - arr_g.size))


        # Observation: [qpos (nq), qvel (nv), ee_pose (7), init_ee_pose (7), goal_pose (7)]
        # obs_dim = self.model.nq + self.model.nv + 7 + 7 + 7
        obs_dim = self.model.nq + self.model.nv + self.model.nq + self.model.nq
        self.observation_space = spaces.Box(
            -np.inf, np.inf, shape=(obs_dim,), dtype=np.float32
        )

        # Action: direct control of joint torques or velocities (size = nu)
        self.action_space = spaces.Box(
            -1.0, 1.0, shape=(self.model.nu,), dtype=np.float32
        )

        # Reward weights
        # self.alpha = 1.0    # position error weight
        # self.beta = 0.01    # velocity penalty
        # self.gamma = 0.001  # jerk penalty

        # â˜… ë…¼ë¬¸ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì„¤ì • â˜…
        # - Joint ëª©í‘œ: ê°•í•˜ê²Œ ìœ ë„ (Î± = 5 ~ 10)  
        # - ì†ë„ íŒ¨ë„í‹°: ì‘ê²Œ (Î³ = 0.001 ~ 0.01)  
        # - jerk íŒ¨ë„í‹°: ë§¤ìš° ì‘ê²Œ (Î´ = 1e-4 ~ 1e-3)

        self.alpha = 50.0    # position error weight
        self.beta = 0.000    # velocity penalty
        self.gamma = 0.0000  # jerk penalty

        # Episode limits
        self.max_steps = 200 # trajectory ê¸¸ì´,         100stepê¹Œì§€ë§Œ í–ˆê¸°ë•Œë¬¸ì— - 3.3ì´ˆ
        self.step_count = 0
        self.last_action = np.zeros(self.model.nu)

        # ë Œë”ëŸ¬ ì´ˆê¸°í™”
        self.viewer = None
        self.renderer = None

    def render(self):  # gymnasiumì€ render()ì— mode íŒŒë¼ë¯¸í„°ê°€ ì—†ìŒ
        mode = self.render_mode  # render_mode ì‚¬ìš©

        if mode == "human":
            if self.viewer is None:
                from mujoco import viewer
                self.viewer = viewer.launch_passive(self.model, self.data)
            self.viewer.sync()  # Sync viewer with data

        # rgb_array ëª¨ë“œ: í”½ì…€ ë°°ì—´(í”„ë ˆì„) ë¦¬í„´
        elif mode == "rgb_array":
            # MuJoCo ë Œë”ëŸ¬ ì´ˆê¸°í™”
            if self.renderer is None:
                from mujoco import Renderer
                self.renderer = Renderer(self.model, 480, 480)
            
            # ì¤‘ìš”: ë§¤ë²ˆ scene ì—…ë°ì´íŠ¸!
            self.renderer.update_scene(self.data)
            pixels = self.renderer.render()
            return pixels  # (H, W, 3) uint8
        
        return None

    def reset(self, *, seed=None, options=None):  # gymnasium ì‹œê·¸ë‹ˆì²˜
        super().reset(seed=seed)
        
        # Reset joint positions and velocities
        self.data.qpos[:] = self.init_qpos
        self.data.qvel[:] = 0
        mujoco.mj_forward(self.model, self.data)

        # ì´ˆê¸°/ì´ì „ ì˜¤ì°¨ ì„¸íŒ…
        self.init_pos_err = np.linalg.norm(self.init_qpos - self.goal_qpos)
        self.prev_pos_err = self.init_pos_err

        # --- XML ì •ì˜ëœ site ë¡œë¶€í„° ì´ˆê¸° EE pose ì¶”ì¶œ ---
        # lap_start ìœ„ì¹˜/ìì„¸ â†’ self.init_ee_pose ì— ì €ì¥
        # start_pos = self.data.site_xpos[self.start_site_id].copy()
        # quat_buf  = np.zeros(4, dtype=np.float64)
        # mujoco.mju_mat2Quat(quat_buf, self.data.site_xmat[self.start_site_id])
        # self.init_ee_pose = np.concatenate([start_pos, quat_buf])

        # # --- goal_pose ì„¤ì • ---
        # if self.goal_pose is None:
        #     # lap_end ìœ„ì¹˜/ìì„¸ â†’ self.goal_pose ì— ì €ì¥
        #     end_pos   = self.data.site_xpos[self.end_site_id].copy()
        #     quat_buf2 = np.zeros(4, dtype=np.float64)
        #     mujoco.mju_mat2Quat(quat_buf2, self.data.site_xmat[self.end_site_id])
        #     self.goal_pose = np.concatenate([end_pos, quat_buf2])

        # reset counters
        self.step_count = 0
        self.last_action[:] = 0
        
        info = {"goal_qpos": self.goal_qpos.copy()}
        return self._get_obs(), info  # gymnasiumì€ (obs, info) íŠœí”Œ ë°˜í™˜

    def _get_obs(self):
        qpos = self.data.qpos.copy()
        qvel = self.data.qvel.copy()
        # ee_pos = self.data.site_xpos[self.ee_site_id].copy()
        # # Convert rotation matrix to quaternion for observation
        # mat = self.data.site_xmat[self.ee_site_id]
        # quat_buf = np.zeros(4, dtype=np.float64)
        # mujoco.mju_mat2Quat(quat_buf, mat)
        # ee_quat = quat_buf.copy()
        # return np.concatenate([
        #     qpos, qvel, ee_pos, ee_quat, self.init_ee_pose, self.goal_pose
        # ])
        return np.concatenate([qpos, qvel, self.goal_qpos, self.init_qpos])

    def step(self, action):
        # Clip and apply action
        action = np.clip(action, self.action_space.low, self.action_space.high)
        self.data.ctrl[:] = action

        # Step the simulation
        mujoco.mj_step(self.model, self.data)
        obs = self._get_obs()

        # 1) Joint-space error (ì£¼ëª©í‘œ) -  Compute joint-space distance
        pos_err = np.linalg.norm(self.data.qpos - self.goal_qpos)
        # 2) ì†ë„ ë° ì œì–´ ë³€í™” íŒ¨ë„í‹°
        vel = np.linalg.norm(self.data.qvel)
        jerk = np.linalg.norm(action - self.last_action)

        # Shaped reward to encourage movement
        reward = (
            - self.alpha * pos_err #pos_err_reward
            - self.beta * vel**2
            - self.gamma * jerk**2
        )

        # â†’ ì—¬ê¸°ì— progress ë³´ìƒ ì¶”ê°€
        progress = (self.prev_pos_err - pos_err) / (self.init_pos_err + 1e-6)
        reward  += self.kappa * progress

        # ëª©í‘œ ë„ë‹¬ ì„±ê³µì‹œ ë³´ë„ˆìŠ¤
        required_pose_err = 0.05 # 0.05 ê¸°ì¤€
        if pos_err < required_pose_err: 
            reward += 50.0 #ì›ë˜ 10.0ì—ì„œ ìˆ˜ì •
            print(f"ğŸ‰ Goal reached! pos_error: {pos_err:.3f}")

        # ë‹¤ìŒ ìŠ¤í…ì„ ìœ„í•´ ì´ì „ ì˜¤ì°¨ ì—…ë°ì´íŠ¸
        self.prev_pos_err = pos_err

        reward = float(np.clip(reward, -1.0, 1.0)) #clipë„ -1.0, 1.0ì€ í•˜ëŠ”ê²Œ ë‚˜ì€ë“¯

        self.last_action = action.copy()
        self.step_count += 1

        # gymnasiumì€ terminatedì™€ truncatedë¥¼ êµ¬ë¶„
        terminated = bool(pos_err < required_pose_err)  # ëª©í‘œ ë„ë‹¬
        truncated = bool(self.step_count >= self.max_steps)  # ì‹œê°„ ì´ˆê³¼

        info = {
            "pos_error": float(pos_err),
            "success": terminated
        }
        return obs, reward, terminated, truncated, info  # 5ê°œ ë°˜í™˜!
    
    def close(self):
        if self.viewer is not None:
            self.viewer.close()
            self.viewer = None
        
        if self.renderer is not None:
            del self.renderer
            self.renderer = None
