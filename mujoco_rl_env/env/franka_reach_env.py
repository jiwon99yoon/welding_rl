
# ì˜ìƒ ìµœëŒ€ê¸¸ì´ 4ì´ˆ, ì´ˆê¸°ê°’ 0ìœ¼ë¡œ ì„¤ì •, xmlíŒŒì¼ì—ì„œ lap_startí‚¤í”„ë ˆì„ jointê°’ ì•ˆì„¤ì •ë˜ì–´ìˆìŒ.
# rolloutì•ˆë³´ì„, ì—í”¼ì†Œë“œ ì œëŒ€ë¡œ ì¢…ë£Œ x, videoì´ë¦„, íŒŒì¼ ê´€ë¦¬ ì•ˆë¨ (ë®ì–´ì“°ê¸°)
# ë¬¸ì œ ë‹¤ í•´ê²°
# /home/minjun/rl_ws/src/mujoco_rl_env/env/franka_reach_env.py
import gymnasium as gym  # gym â†’ gymnasiumìœ¼ë¡œ ë³€ê²½!
from gymnasium import spaces
import numpy as np
import mujoco

class FrankaReachEnv(gym.Env):
    metadata = {"render_modes": ["human", "rgb_array"]}
    """
    Gym environment for moving a Franka Panda arm in MuJoCo
    from an initial joint configuration to a goal pose, with a reward
    that encourages smooth, accurate motion.
    """
    def __init__(self, xml_path, init_qpos=None, goal_qpos=None, render_mode=None):
        super().__init__()
        # Load model and data
        self.model = mujoco.MjModel.from_xml_path(xml_path)
        self.data = mujoco.MjData(self.model)

        # Progress reward ê³„ìˆ˜
        self.kappa = 5.0

        # reset() ì‹œ ì„¸íŒ…ë  ê°’ë“¤
        self.init_pos_err = None
        self.prev_pos_err = None

        # # --- lap_start, lap_end site ID ì¡°íšŒ ---
        # self.start_site_id = mujoco.mj_name2id(
        #     self.model, mujoco.mjtObj.mjOBJ_SITE, "lap_start"
        # )
        # self.end_site_id = mujoco.mj_name2id(
        #     self.model, mujoco.mjtObj.mjOBJ_SITE, "lap_end"
        # )

        # render_modeì„¤ì •!
        self.render_mode = render_mode

        # Initial joint positions
        if init_qpos is None:
            # default to zeros if no keyframes
            self.init_qpos = np.zeros(self.model.nq)
        else:
            # ì…ë ¥ë°›ì€ init_qposê°€ nqë³´ë‹¤ ì‘ìœ¼ë©´ 0ìœ¼ë¡œ íŒ¨ë”©
            if len(init_qpos) < self.model.nq:
                self.init_qpos = np.zeros(self.model.nq)
                self.init_qpos[:len(init_qpos)] = init_qpos
            else:
                self.init_qpos = np.array(init_qpos[:self.model.nq])

        # Fixed goal pose [x, y, z, qx, qy, qz, qw]
        # goal pose -> goal jointstateë¡œ ë³€ê²½
        # self.goal_pose = np.array(goal_pose) if goal_pose is not None else None
        # Identify end-effector site (must exist in XML as <site name="ee_site" ...>)
        # self.ee_site_id = mujoco.mj_name2id(
        #     self.model, mujoco.mjtObj.mjOBJ_SITE, "ee_site"
        # )

        if goal_qpos is None:
            # sample once at init
            low = self.model.jnt_range[:, 0].astype(np.float32)
            high = self.model.jnt_range[:, 1].astype(np.float32)
            self.goal_qpos = np.random.uniform(low, high)
        else:
            arr_g = np.array(goal_qpos, dtype=np.float32)
            self.goal_qpos = arr_g[:self.model.nq] if arr_g.size >= self.model.nq else np.pad(arr_g, (0, self.model.nq - arr_g.size))


        # Observation: [qpos (nq), qvel (nv), ee_pose (7), init_ee_pose (7), goal_pose (7)]
        # obs_dim = self.model.nq + self.model.nv + 7 + 7 + 7
        obs_dim = self.model.nq + self.model.nv + self.model.nq + self.model.nq
        self.observation_space = spaces.Box(
            -np.inf, np.inf, shape=(obs_dim,), dtype=np.float32
        )

        # Action: direct control of joint torques or velocities (size = nu)
        self.action_space = spaces.Box(
            -1.0, 1.0, shape=(self.model.nu,), dtype=np.float32
        )

        # Reward weights
        # self.alpha = 1.0    # position error weight
        # self.beta = 0.01    # velocity penalty
        # self.gamma = 0.001  # jerk penalty

        # â˜… ë…¼ë¬¸ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì„¤ì • â˜…
        # - Joint ëª©í‘œ: ê°•í•˜ê²Œ ìœ ë„ (Î± = 5 ~ 10)  
        # - EE ìœ„ì¹˜: ë³´ì¡° ë³´ìƒ (Î² = 1 ~ 3) -> ì œê±°
        # - ì†ë„ íŒ¨ë„í‹°: ì‘ê²Œ (Î³ = 0.001 ~ 0.01)  
        # - jerk íŒ¨ë„í‹°: ë§¤ìš° ì‘ê²Œ (Î´ = 1e-4 ~ 1e-3)

        self.alpha = 10.0    # position error weight
        self.beta = 0.01    # velocity penalty
        self.gamma = 0.0001  # jerk penalty

        # Episode limits
        self.max_steps = 200 # trajectory ê¸¸ì´, 100stepê¹Œì§€ë§Œ í–ˆê¸°ë•Œë¬¸ì— - 3.3ì´ˆ
        self.step_count = 0
        self.last_action = np.zeros(self.model.nu)

        # ë Œë”ëŸ¬ ì´ˆê¸°í™”
        self.viewer = None
        self.renderer = None

    def render(self):  # gymnasiumì€ render()ì— mode íŒŒë¼ë¯¸í„°ê°€ ì—†ìŒ
        mode = self.render_mode  # render_mode ì‚¬ìš©

        if mode == "human":
            if self.viewer is None:
                from mujoco import viewer
                self.viewer = viewer.launch_passive(self.model, self.data)
            self.viewer.sync()  # Sync viewer with data

        # rgb_array ëª¨ë“œ: í”½ì…€ ë°°ì—´(í”„ë ˆì„) ë¦¬í„´
        elif mode == "rgb_array":
            # MuJoCo ë Œë”ëŸ¬ ì´ˆê¸°í™”
            if self.renderer is None:
                from mujoco import Renderer
                self.renderer = Renderer(self.model, 480, 480)
            
            # ì¤‘ìš”: ë§¤ë²ˆ scene ì—…ë°ì´íŠ¸!
            self.renderer.update_scene(self.data)
            pixels = self.renderer.render()
            return pixels  # (H, W, 3) uint8
        
        return None

    def reset(self, *, seed=None, options=None):  # gymnasium ì‹œê·¸ë‹ˆì²˜
        super().reset(seed=seed)
        
        # Reset joint positions and velocities
        self.data.qpos[:] = self.init_qpos
        self.data.qvel[:] = 0
        mujoco.mj_forward(self.model, self.data)

        # ì´ˆê¸°/ì´ì „ ì˜¤ì°¨ ì„¸íŒ…
        self.init_pos_err = np.linalg.norm(self.init_qpos - self.goal_qpos)
        self.prev_pos_err = self.init_pos_err

        # --- XML ì •ì˜ëœ site ë¡œë¶€í„° ì´ˆê¸° EE pose ì¶”ì¶œ ---
        # lap_start ìœ„ì¹˜/ìì„¸ â†’ self.init_ee_pose ì— ì €ì¥
        # start_pos = self.data.site_xpos[self.start_site_id].copy()
        # quat_buf  = np.zeros(4, dtype=np.float64)
        # mujoco.mju_mat2Quat(quat_buf, self.data.site_xmat[self.start_site_id])
        # self.init_ee_pose = np.concatenate([start_pos, quat_buf])

        # # --- goal_pose ì„¤ì • ---
        # if self.goal_pose is None:
        #     # lap_end ìœ„ì¹˜/ìì„¸ â†’ self.goal_pose ì— ì €ì¥
        #     end_pos   = self.data.site_xpos[self.end_site_id].copy()
        #     quat_buf2 = np.zeros(4, dtype=np.float64)
        #     mujoco.mju_mat2Quat(quat_buf2, self.data.site_xmat[self.end_site_id])
        #     self.goal_pose = np.concatenate([end_pos, quat_buf2])

        # reset counters
        self.step_count = 0
        self.last_action[:] = 0
        
        info = {"goal_qpos": self.goal_qpos.copy()}
        return self._get_obs(), info  # gymnasiumì€ (obs, info) íŠœí”Œ ë°˜í™˜

    def _get_obs(self):
        qpos = self.data.qpos.copy()
        qvel = self.data.qvel.copy()
        # ee_pos = self.data.site_xpos[self.ee_site_id].copy()
        # # Convert rotation matrix to quaternion for observation
        # mat = self.data.site_xmat[self.ee_site_id]
        # quat_buf = np.zeros(4, dtype=np.float64)
        # mujoco.mju_mat2Quat(quat_buf, mat)
        # ee_quat = quat_buf.copy()
        # return np.concatenate([
        #     qpos, qvel, ee_pos, ee_quat, self.init_ee_pose, self.goal_pose
        # ])
        return np.concatenate([qpos, qvel, self.goal_qpos, self.init_qpos])

    def step(self, action):
        # Clip and apply action
        action = np.clip(action, self.action_space.low, self.action_space.high)
        self.data.ctrl[:] = action

        # Step the simulation
        mujoco.mj_step(self.model, self.data)
        obs = self._get_obs()


        # â–¶ pos_err: ì „ì²´ ê´€ì ˆ(qpos) í‰ê·  L2 ê±°ë¦¬
        # â–¶ vel_pen: ê´€ì ˆ ì†ë„ ì œê³±í•© (energy cost)
        # â–¶ act_pen: ì œì–´ ì…ë ¥ ë³€í™”(jerk) ì œê³±í•©
        # 1) Joint-space error (ì£¼ëª©í‘œ) -  Compute joint-space distance
        pos_err = np.linalg.norm(self.data.qpos - self.goal_qpos)
        # 2) ì†ë„ ë° ì œì–´ ë³€í™” íŒ¨ë„í‹°
        vel = np.linalg.norm(self.data.qvel)
        jerk = np.linalg.norm(action - self.last_action)

        # Shaped reward to encourage movement
        reward = (
            - self.alpha * pos_err #pos_err_reward
            - self.beta * vel**2
            - self.gamma * jerk**2
        )

        # â†’ ì—¬ê¸°ì— progress ë³´ìƒ ì¶”ê°€
        progress = (self.prev_pos_err - pos_err) / (self.init_pos_err + 1e-6)
        reward  += self.kappa * progress

        # ëª©í‘œ ë„ë‹¬ ì„±ê³µì‹œ ë³´ë„ˆìŠ¤
        required_pose_err = 0.05 # 0.05 ê¸°ì¤€
        if pos_err < required_pose_err:  # pos_errê°€ 0.001ë¡œ ìˆ˜ì • (ë” í•©ë¦¬ì ì¸ ê°’)
            reward += 50.0 #ì›ë˜ 10.0
            print(f"ğŸ‰ Goal reached! pos_error: {pos_err:.3f}")

        # ë‹¤ìŒ ìŠ¤í…ì„ ìœ„í•´ ì´ì „ ì˜¤ì°¨ ì—…ë°ì´íŠ¸
        self.prev_pos_err = pos_err

        reward = float(np.clip(reward, -1.0, 1.0)) #clipë„ -1.0, 1.0ì€ í•˜ëŠ”ê²Œ ë‚˜ì€ë“¯

        self.last_action = action.copy()
        self.step_count += 1

        # gymnasiumì€ terminatedì™€ truncatedë¥¼ êµ¬ë¶„
        terminated = bool(pos_err < required_pose_err)  # ëª©í‘œ ë„ë‹¬
        truncated = bool(self.step_count >= self.max_steps)  # ì‹œê°„ ì´ˆê³¼

        info = {
            "pos_error": float(pos_err),
            "success": terminated
        }
        return obs, reward, terminated, truncated, info  # 5ê°œ ë°˜í™˜!
    
    def close(self):
        if self.viewer is not None:
            self.viewer.close()
            self.viewer = None
        
        if self.renderer is not None:
            del self.renderer
            self.renderer = None

#------------------------------------------------------------------------------------------------------------------



# /home/minjun/rl_ws/src/mujoco_rl_env/env/franka_reach_env.py
# import gymnasium as gym
# from gymnasium import spaces
# import numpy as np
# import mujoco

# class FrankaReachEnv(gym.Env):
#     metadata = {"render_modes": ["human", "rgb_array"]}
#     """
#     Gym environment for moving a Franka Panda arm in MuJoCo
#     from an initial joint configuration to a goal configuration.
#     """
#     def __init__(self, xml_path, init_qpos=None, goal_qpos=None, render_mode=None):
#         super().__init__()
#         # Load model and data
#         self.model = mujoco.MjModel.from_xml_path(xml_path)
#         self.data = mujoco.MjData(self.model)

#         self.render_mode = render_mode

#         # Initial joint positions (ì „ì²´ qpos)
#         if init_qpos is None:
#             self.init_qpos = np.zeros(self.model.nq)
#         else:
#             # ì…ë ¥ë°›ì€ init_qposê°€ nqë³´ë‹¤ ì‘ìœ¼ë©´ 0ìœ¼ë¡œ íŒ¨ë”©
#             if len(init_qpos) < self.model.nq:
#                 self.init_qpos = np.zeros(self.model.nq)
#                 self.init_qpos[:len(init_qpos)] = init_qpos
#             else:
#                 self.init_qpos = np.array(init_qpos[:self.model.nq])

#         # Goal joint positions (7 DOF arm only)
#         if goal_qpos is None:
#             # ê¸°ë³¸ê°’: ì´ˆê¸° ìœ„ì¹˜ì™€ ë™ì¼
#             self.goal_qpos = self.init_qpos[:7].copy()
#         else:
#             self.goal_qpos = np.array(goal_qpos[:7])  # ì²« 7ê°œ jointë§Œ ì‚¬ìš©

#         # Get end-effector body (last link in the arm)
#         self.ee_body_id = self.model.nbody - 1
        
#         # Observation: [qpos (nq), qvel (nv), goal_qpos (7)]
#         obs_dim = self.model.nq + self.model.nv + 7
#         self.observation_space = spaces.Box(
#             -np.inf, np.inf, shape=(obs_dim,), dtype=np.float32
#         )

#         # Action: joint torques
#         self.action_space = spaces.Box(
#             -1.0, 1.0, shape=(self.model.nu,), dtype=np.float32
#         )

#         # Reward weights
#         self.alpha = 10.0    # joint position error weight
#         self.beta = 0.1     # velocity penalty
#         self.gamma = 0.01   # jerk penalty

#         # Episode limits
#         self.max_steps = 300
#         self.step_count = 0
#         self.last_action = np.zeros(self.model.nu)

#         # Renderers
#         self.viewer = None
#         self.renderer = None

#         # For tracking initial and goal end-effector positions
#         self.init_ee_pos = None
#         self.goal_ee_pos = None
        
#         # Debug print frequency
#         self.episode_count = 0

#     def _compute_ee_pos_from_qpos(self, qpos):
#         """ì£¼ì–´ì§„ joint configurationì—ì„œ end-effector ìœ„ì¹˜ ê³„ì‚°"""
#         # ìƒˆë¡œìš´ MjData ì¸ìŠ¤í„´ìŠ¤ ìƒì„±í•˜ì—¬ í˜„ì¬ ìƒíƒœë¥¼ ë³´ì¡´
#         temp_data = mujoco.MjData(self.model)
#         temp_data.qpos[:] = qpos
#         mujoco.mj_forward(self.model, temp_data)
        
#         # End-effector ìœ„ì¹˜ ê°€ì ¸ì˜¤ê¸°
#         ee_pos = temp_data.xpos[self.ee_body_id].copy()
#         return ee_pos

#     def render(self):
#         mode = self.render_mode

#         if mode == "human":
#             if self.viewer is None:
#                 from mujoco import viewer
#                 self.viewer = viewer.launch_passive(self.model, self.data)
#             self.viewer.sync()

#         elif mode == "rgb_array":
#             if self.renderer is None:
#                 from mujoco import Renderer
#                 self.renderer = Renderer(self.model, 480, 480)
            
#             self.renderer.update_scene(self.data)
#             pixels = self.renderer.render()
#             return pixels
        
#         return None

#     def reset(self, *, seed=None, options=None):
#         super().reset(seed=seed)
        
#         # Reset to initial joint configuration
#         self.data.qpos[:] = self.init_qpos
#         self.data.qvel[:] = 0
#         mujoco.mj_forward(self.model, self.data)

#         # Compute initial and goal end-effector positions
#         self.init_ee_pos = self.data.xpos[self.ee_body_id].copy()
        
#         # Goal qposë¡œ goal EE position ê³„ì‚°
#         goal_qpos_full = self.init_qpos.copy()
#         goal_qpos_full[:7] = self.goal_qpos
#         self.goal_ee_pos = self._compute_ee_pos_from_qpos(goal_qpos_full)

#         self.step_count = 0
#         self.last_action[:] = 0
#         self.episode_count += 1
        
#         # Print debug info only every 10 episodes
#         if self.episode_count % 10 == 1:
#             print(f"\nğŸ¯ Reset environment (Episode {self.episode_count}):")
#             print(f"   Initial arm joints: {self.init_qpos[:7]}")
#             print(f"   Goal arm joints: {self.goal_qpos}")
#             print(f"   Initial EE pos: {self.init_ee_pos}")
#             print(f"   Goal EE pos: {self.goal_ee_pos}")
#             print(f"   EE distance to goal: {np.linalg.norm(self.init_ee_pos - self.goal_ee_pos):.3f}")
#             print(f"   Joint distance to goal: {np.linalg.norm(self.init_qpos[:7] - self.goal_qpos):.3f}")
        
#         info = {}
#         return self._get_obs(), info

#     def _get_obs(self):
#         qpos = self.data.qpos.copy()
#         qvel = self.data.qvel.copy()
        
#         return np.concatenate([
#             qpos, qvel, self.goal_qpos
#         ])

#     def step(self, action):
#         # Clip and scale action
#         action = np.clip(action, self.action_space.low, self.action_space.high)
#         self.data.ctrl[:] = action * 5.0

#         # Step simulation
#         mujoco.mj_step(self.model, self.data)

#         obs = self._get_obs()

#         # Compute rewards
#         # 1. Joint position error (main objective)
#         joint_error = np.linalg.norm(self.data.qpos[:7] - self.goal_qpos)
        
#         # 2. End-effector position error (auxiliary)
#         ee_pos = self.data.xpos[self.ee_body_id]
#         ee_error = np.linalg.norm(ee_pos - self.goal_ee_pos)
        
#         # 3. Velocity and jerk penalties
#         vel = np.linalg.norm(self.data.qvel)
#         jerk = np.linalg.norm(action - self.last_action)

#         # Shaped reward
#         joint_reward = np.exp(-joint_error * 2.0) * 5.0  # Joint ì •í™•ë„
#         ee_reward = np.exp(-ee_error * 3.0) * 3.0       # EE ìœ„ì¹˜ ë³´ì¡°
        
#         # Progress reward
#         init_joint_error = np.linalg.norm(self.init_qpos[:7] - self.goal_qpos)
#         joint_progress = (init_joint_error - joint_error) / (init_joint_error + 1e-6)
#         progress_reward = joint_progress * 5.0

#         # Total reward
#         reward = (
#             joint_reward 
#             + ee_reward
#             + progress_reward
#             - self.beta * vel**2
#             - self.gamma * jerk**2
#         )

#         # Success bonus
#         success = joint_error < 0.1 and ee_error < 0.05  # Jointê³¼ EE ëª¨ë‘ ê·¼ì ‘
#         if success:
#             reward += 100.0
#             if self.episode_count % 10 == 1:  # 10 ì—í”¼ì†Œë“œë§ˆë‹¤ë§Œ ì¶œë ¥
#                 print(f"ğŸ‰ Goal reached! Joint error: {joint_error:.3f}, EE error: {ee_error:.3f}")

#         # Clip reward
#         reward = float(np.clip(reward, -10.0, 110.0))

#         self.last_action = action.copy()
#         self.step_count += 1

#         # Episode termination
#         terminated = bool(success)
#         truncated = bool(self.step_count >= self.max_steps)
        
#         info = {
#             "joint_error": float(joint_error),
#             "ee_error": float(ee_error),
#             "success": success
#         }
        
#         return obs, reward, terminated, truncated, info
    
#     def close(self):
#         if self.viewer is not None:
#             self.viewer.close()
#             self.viewer = None
        
#         if self.renderer is not None:
#             del self.renderer
#             self.renderer = None

#------------------------------------------------------------------------------------------------------------------
# qposê¸°ë°˜ìœ¼ë¡œ ee pose inverse kinematic ê³„ì‚° -> ìˆ˜ì •
# ë°‘ì—êº¼ ì½”ë“œ ì˜¤ë¥˜ ì´ˆê¸° eeì™€ ë‚˜ì¤‘ eeê°™ë‹¤ê³ í•¨ forward kinematicì˜ ì‘ë™ x
# /home/minjun/rl_ws/src/mujoco_rl_env/env/franka_reach_env.py
# import gymnasium as gym
# from gymnasium import spaces
# import numpy as np
# import mujoco

# class FrankaReachEnv(gym.Env):
#     metadata = {"render_modes": ["human", "rgb_array"]}
#     """
#     Gym environment for moving a Franka Panda arm in MuJoCo
#     from an initial joint configuration to a goal configuration.
#     """
#     def __init__(self, xml_path, init_qpos=None, goal_qpos=None, render_mode=None):
#         super().__init__()
#         # Load model and data
#         self.model = mujoco.MjModel.from_xml_path(xml_path)
#         self.data = mujoco.MjData(self.model)

#         self.render_mode = render_mode

#         # Initial joint positions (ì „ì²´ qpos)
#         if init_qpos is None:
#             self.init_qpos = np.zeros(self.model.nq)
#         else:
#             # ì…ë ¥ë°›ì€ init_qposê°€ nqë³´ë‹¤ ì‘ìœ¼ë©´ 0ìœ¼ë¡œ íŒ¨ë”©
#             if len(init_qpos) < self.model.nq:
#                 self.init_qpos = np.zeros(self.model.nq)
#                 self.init_qpos[:len(init_qpos)] = init_qpos
#             else:
#                 self.init_qpos = np.array(init_qpos[:self.model.nq])

#         # Goal joint positions (7 DOF arm only)
#         if goal_qpos is None:
#             # ê¸°ë³¸ê°’: ì´ˆê¸° ìœ„ì¹˜ì™€ ë™ì¼
#             self.goal_qpos = self.init_qpos[:7].copy()
#         else:
#             self.goal_qpos = np.array(goal_qpos[:7])  # ì²« 7ê°œ jointë§Œ ì‚¬ìš©

#         # Get end-effector body (last link in the arm)
#         self.ee_body_id = self.model.nbody - 1
        
#         # Observation: [qpos (nq), qvel (nv), goal_qpos (7)]
#         obs_dim = self.model.nq + self.model.nv + 7
#         self.observation_space = spaces.Box(
#             -np.inf, np.inf, shape=(obs_dim,), dtype=np.float32
#         )

#         # Action: joint torques
#         self.action_space = spaces.Box(
#             -1.0, 1.0, shape=(self.model.nu,), dtype=np.float32
#         )

#         # Reward weights
#         self.alpha = 10.0    # joint position error weight
#         self.beta = 0.1     # velocity penalty
#         self.gamma = 0.01   # jerk penalty

#         # Episode limits
#         self.max_steps = 300
#         self.step_count = 0
#         self.last_action = np.zeros(self.model.nu)

#         # Renderers
#         self.viewer = None
#         self.renderer = None

#         # For tracking initial and goal end-effector positions
#         self.init_ee_pos = None
#         self.goal_ee_pos = None

#     def _compute_ee_pos_from_qpos(self, qpos):
#         """ì£¼ì–´ì§„ joint configurationì—ì„œ end-effector ìœ„ì¹˜ ê³„ì‚°"""
#         # ì„ì‹œë¡œ qpos ì„¤ì •í•˜ì—¬ forward kinematics ê³„ì‚°
#         old_qpos = self.data.qpos.copy()
#         self.data.qpos[:] = qpos
#         mujoco.mj_forward(self.model, self.data)
#         ee_pos = self.data.xpos[self.ee_body_id].copy()
#         # ì›ë˜ ìƒíƒœë¡œ ë³µì›
#         self.data.qpos[:] = old_qpos
#         mujoco.mj_forward(self.model, self.data)
#         return ee_pos

#     def render(self):
#         mode = self.render_mode

#         if mode == "human":
#             if self.viewer is None:
#                 from mujoco import viewer
#                 self.viewer = viewer.launch_passive(self.model, self.data)
#             self.viewer.sync()

#         elif mode == "rgb_array":
#             if self.renderer is None:
#                 from mujoco import Renderer
#                 self.renderer = Renderer(self.model, 480, 480)
            
#             self.renderer.update_scene(self.data)
#             pixels = self.renderer.render()
#             return pixels
        
#         return None

#     def reset(self, *, seed=None, options=None):
#         super().reset(seed=seed)
        
#         # Reset to initial joint configuration
#         self.data.qpos[:] = self.init_qpos
#         self.data.qvel[:] = 0
#         mujoco.mj_forward(self.model, self.data)

#         # Compute initial and goal end-effector positions
#         self.init_ee_pos = self.data.xpos[self.ee_body_id].copy()
        
#         # Goal qposë¡œ goal EE position ê³„ì‚°
#         goal_qpos_full = self.init_qpos.copy()
#         goal_qpos_full[:7] = self.goal_qpos
#         self.goal_ee_pos = self._compute_ee_pos_from_qpos(goal_qpos_full)

#         self.step_count = 0
#         self.last_action[:] = 0
        
#         # Print debug info
#         print(f"\nğŸ¯ Reset environment:")
#         print(f"   Initial arm joints: {self.init_qpos[:7]}")
#         print(f"   Goal arm joints: {self.goal_qpos}")
#         print(f"   Initial EE pos: {self.init_ee_pos}")
#         print(f"   Goal EE pos: {self.goal_ee_pos}")
#         print(f"   EE distance to goal: {np.linalg.norm(self.init_ee_pos - self.goal_ee_pos):.3f}")
        
#         info = {}
#         return self._get_obs(), info

#     def _get_obs(self):
#         qpos = self.data.qpos.copy()
#         qvel = self.data.qvel.copy()
        
#         return np.concatenate([
#             qpos, qvel, self.goal_qpos
#         ])

#     def step(self, action):
#         # Clip and scale action
#         action = np.clip(action, self.action_space.low, self.action_space.high)
#         self.data.ctrl[:] = action * 5.0

#         # Step simulation
#         mujoco.mj_step(self.model, self.data)

#         obs = self._get_obs()

#         # Compute rewards
#         # 1. Joint position error (main objective)
#         joint_error = np.linalg.norm(self.data.qpos[:7] - self.goal_qpos)
        
#         # 2. End-effector position error (auxiliary)
#         ee_pos = self.data.xpos[self.ee_body_id]
#         ee_error = np.linalg.norm(ee_pos - self.goal_ee_pos)
        
#         # 3. Velocity and jerk penalties
#         vel = np.linalg.norm(self.data.qvel)
#         jerk = np.linalg.norm(action - self.last_action)

#         # Shaped reward
#         joint_reward = np.exp(-joint_error * 2.0) * 5.0  # Joint ì •í™•ë„
#         ee_reward = np.exp(-ee_error * 3.0) * 3.0       # EE ìœ„ì¹˜ ë³´ì¡°
        
#         # Progress reward
#         init_joint_error = np.linalg.norm(self.init_qpos[:7] - self.goal_qpos)
#         joint_progress = (init_joint_error - joint_error) / (init_joint_error + 1e-6)
#         progress_reward = joint_progress * 5.0

#         # Total reward
#         reward = (
#             joint_reward 
#             + ee_reward
#             + progress_reward
#             - self.beta * vel**2
#             - self.gamma * jerk**2
#         )

#         # Success bonus
#         success = joint_error < 0.1 and ee_error < 0.05  # Jointê³¼ EE ëª¨ë‘ ê·¼ì ‘
#         if success:
#             reward += 100.0
#             print(f"ğŸ‰ Goal reached! Joint error: {joint_error:.3f}, EE error: {ee_error:.3f}")

#         # Clip reward
#         reward = float(np.clip(reward, -10.0, 110.0))

#         self.last_action = action.copy()
#         self.step_count += 1

#         # Episode termination
#         terminated = bool(success)
#         truncated = bool(self.step_count >= self.max_steps)
        
#         info = {
#             "joint_error": float(joint_error),
#             "ee_error": float(ee_error),
#             "success": success
#         }
        
#         return obs, reward, terminated, truncated, info
    
#     def close(self):
#         if self.viewer is not None:
#             self.viewer.close()
#             self.viewer = None
        
#         if self.renderer is not None:
#             del self.renderer
#             self.renderer = None


#------------------------------------------------------------------------------------------------------------------
# 20250608 1717 ìˆ˜ì •ì™„ë£Œ
# ì¼ë‹¨ claude ì‹ìœ¼ë¡œ ë³€ê²½ í›„, ì´ˆê¸° qposì°¾ì - mujocoì—ì„œ!
# # /home/minjun/rl_ws/src/mujoco_rl_env/env/franka_reach_env.py
# import gymnasium as gym
# from gymnasium import spaces
# import numpy as np
# import mujoco

# class FrankaReachEnv(gym.Env):
#     metadata = {"render_modes": ["human", "rgb_array"]}
#     """
#     Gym environment for moving a Franka Panda arm in MuJoCo
#     from lap_start to lap_end, with proper initialization.
#     """
#     def __init__(self, xml_path, init_qpos=None, goal_pose=None, render_mode=None):
#         super().__init__()
#         # Load model and data
#         self.model = mujoco.MjModel.from_xml_path(xml_path)
#         self.data = mujoco.MjData(self.model)

#         # Get site IDs
#         self.start_site_id = mujoco.mj_name2id(
#             self.model, mujoco.mjtObj.mjOBJ_SITE, "lap_start"
#         )
#         self.end_site_id = mujoco.mj_name2id(
#             self.model, mujoco.mjtObj.mjOBJ_SITE, "lap_end"
#         )

#         self.render_mode = render_mode

#         # Get keyframe qpos for lap_start if available and init_qpos not provided
#         if init_qpos is None:
#             # Try to get lap_start keyframe
#             try:
#                 keyframe_id = mujoco.mj_name2id(
#                     self.model, mujoco.mjtObj.mjOBJ_KEY, "lap_start"
#                 )
#                 if keyframe_id >= 0:
#                     # Use lap_start keyframe joint positions
#                     self.init_qpos = self.model.key_qpos[keyframe_id].copy()
#                     print(f"âœ… Using lap_start keyframe for initial pose")
#                 else:
#                     self.init_qpos = np.zeros(self.model.nq)
#                     print(f"âš ï¸  No lap_start keyframe found, using zeros")
#             except:
#                 self.init_qpos = np.zeros(self.model.nq)
#                 print(f"âš ï¸  Could not find lap_start keyframe, using zeros")
#         else:
#             # Use provided init_qpos
#             if len(init_qpos) < self.model.nq:
#                 self.init_qpos = np.zeros(self.model.nq)
#                 self.init_qpos[:len(init_qpos)] = init_qpos
#             else:
#                 self.init_qpos = np.array(init_qpos[:self.model.nq])

#         # Goal pose
#         self.goal_pose = np.array(goal_pose) if goal_pose is not None else None

#         # For tracking end-effector, we'll use the last link instead of ee_site
#         # Assuming the last body is the end-effector
#         self.ee_body_id = self.model.nbody - 1  # Last body in the kinematic chain
        
#         # Observation: [qpos (nq), qvel (nv), ee_pose (7), init_ee_pose (7), goal_pose (7)]
#         obs_dim = self.model.nq + self.model.nv + 7 + 7 + 7
#         self.observation_space = spaces.Box(
#             -np.inf, np.inf, shape=(obs_dim,), dtype=np.float32
#         )

#         # Action: joint torques
#         self.action_space = spaces.Box(
#             -1.0, 1.0, shape=(self.model.nu,), dtype=np.float32
#         )

#         # Reward weights
#         self.alpha = 10.0    # position error weight (increased)
#         self.beta = 0.1     # velocity penalty (increased)
#         self.gamma = 0.01   # jerk penalty (increased)

#         # Episode limits
#         self.max_steps = 300  # Increased from 100 to 300 for longer episodes
#         self.step_count = 0
#         self.last_action = np.zeros(self.model.nu)

#         # Renderers
#         self.viewer = None
#         self.renderer = None

#     def _get_ee_pose(self):
#         """Get end-effector pose using body position instead of site"""
#         # Get body position
#         ee_pos = self.data.xpos[self.ee_body_id].copy()
        
#         # Get body orientation matrix and convert to quaternion
#         mat = self.data.xmat[self.ee_body_id].reshape(3, 3)
#         quat_buf = np.zeros(4, dtype=np.float64)
#         mujoco.mju_mat2Quat(quat_buf, mat)
        
#         return np.concatenate([ee_pos, quat_buf])

#     def render(self):
#         mode = self.render_mode

#         if mode == "human":
#             if self.viewer is None:
#                 from mujoco import viewer
#                 self.viewer = viewer.launch_passive(self.model, self.data)
#             self.viewer.sync()

#         elif mode == "rgb_array":
#             if self.renderer is None:
#                 from mujoco import Renderer
#                 self.renderer = Renderer(self.model, 480, 480)
            
#             self.renderer.update_scene(self.data)
#             pixels = self.renderer.render()
#             return pixels
        
#         return None

#     def reset(self, *, seed=None, options=None):
#         super().reset(seed=seed)
        
#         # Reset joint positions to lap_start configuration
#         self.data.qpos[:] = self.init_qpos
#         self.data.qvel[:] = 0
#         mujoco.mj_forward(self.model, self.data)

#         # Get initial EE pose from lap_start site
#         start_pos = self.data.site_xpos[self.start_site_id].copy()
#         quat_buf = np.zeros(4, dtype=np.float64)
#         mujoco.mju_mat2Quat(quat_buf, self.data.site_xmat[self.start_site_id])
#         self.init_ee_pose = np.concatenate([start_pos, quat_buf])

#         # Set goal pose from lap_end site
#         if self.goal_pose is None:
#             end_pos = self.data.site_xpos[self.end_site_id].copy()
#             quat_buf2 = np.zeros(4, dtype=np.float64)
#             mujoco.mju_mat2Quat(quat_buf2, self.data.site_xmat[self.end_site_id])
#             self.goal_pose = np.concatenate([end_pos, quat_buf2])

#         self.step_count = 0
#         self.last_action[:] = 0
        
#         # Print debug info
#         print(f"ğŸ¯ Initial EE pos: {self.init_ee_pose[:3]}")
#         print(f"ğŸ¯ Goal pos: {self.goal_pose[:3]}")
#         print(f"ğŸ¯ Distance to goal: {np.linalg.norm(self.init_ee_pose[:3] - self.goal_pose[:3]):.3f}")
        
#         info = {}
#         return self._get_obs(), info

#     def _get_obs(self):
#         qpos = self.data.qpos.copy()
#         qvel = self.data.qvel.copy()
#         ee_pose = self._get_ee_pose()
        
#         return np.concatenate([
#             qpos, qvel, ee_pose, self.init_ee_pose, self.goal_pose
#         ])

#     def step(self, action):
#         # Clip and scale action
#         action = np.clip(action, self.action_space.low, self.action_space.high)
#         # Scale up the action for more aggressive movement
#         self.data.ctrl[:] = action * 5.0  # Increased action scale

#         # Step simulation
#         mujoco.mj_step(self.model, self.data)

#         obs = self._get_obs()

#         # Compute reward
#         ee_pose = self._get_ee_pose()
#         ee_pos = ee_pose[:3]
#         goal_pos = self.goal_pose[:3]
        
#         # Distance to goal
#         dist = np.linalg.norm(ee_pos - goal_pos)
        
#         # Velocity and jerk penalties
#         vel = np.linalg.norm(self.data.qvel)
#         jerk = np.linalg.norm(action - self.last_action)

#         # Shaped reward to encourage movement
#         dist_reward = np.exp(-dist * 5.0)  # Exponential distance reward
        
#         # Progress reward - reward getting closer to goal
#         prev_dist = np.linalg.norm(self.init_ee_pose[:3] - goal_pos)
#         progress = (prev_dist - dist) / prev_dist
#         progress_reward = progress * 10.0

#         # Total reward
#         reward = (
#             dist_reward * self.alpha
#             + progress_reward
#             - self.beta * vel**2
#             - self.gamma * jerk**2
#         )

#         # Success bonus
#         if dist < 0.02:  # 2cm threshold
#             reward += 100.0
#             print(f"ğŸ‰ Goal reached! Distance: {dist:.3f}")

#         # Clip reward
#         reward = float(np.clip(reward, -10.0, 110.0))

#         self.last_action = action.copy()
#         self.step_count += 1

#         # Episode termination
#         terminated = bool(dist < 0.02)  # Success
#         truncated = bool(self.step_count >= self.max_steps)  # Timeout
        
#         info = {
#             "distance": float(dist),
#             "success": terminated,
#             "progress": float(progress)
#         }
        
#         return obs, reward, terminated, truncated, info
    
#     def close(self):
#         if self.viewer is not None:
#             self.viewer.close()
#             self.viewer = None
        
#         if self.renderer is not None:
#             del self.renderer
#             self.renderer = None





#------------------------------------------------------------------------------------------------------------------


#------------------------------------------------------------------------------------------------------------------
# 2025.06.08 13:01
# import gym
# from gym import spaces
# import numpy as np
# import mujoco

# class FrankaReachEnv(gym.Env):
#     metadata = {"render_modes": ["human", "rgb_array"]}  # â† ì¶”ê°€
#     """
#     Gym environment for moving a Franka Panda arm in MuJoCo
#     from an initial joint configuration to a goal pose, with a reward
#     that encourages smooth, accurate motion.
#     """
#     def __init__(self, xml_path, init_qpos=None, goal_pose=None, render_mode=None):
#         super().__init__()
#         # Load model and data
#         self.model = mujoco.MjModel.from_xml_path(xml_path)
#         self.data = mujoco.MjData(self.model)

#         # --- lap_start, lap_end site ID ì¡°íšŒ ---
#         #   XML ìƒ <site name="lap_start"/> ì™€ <site name="lap_end"/> ì˜ ID
#         self.start_site_id = mujoco.mj_name2id(
#             self.model, mujoco.mjtObj.mjOBJ_SITE, "lap_start"
#         )
#         self.end_site_id = mujoco.mj_name2id(
#             self.model, mujoco.mjtObj.mjOBJ_SITE, "lap_end"
#         )

#         # render_modeì„¤ì •!
#         self.render_mode = render_mode

#         # Initial joint positions
#         if init_qpos is None:
#             # default to zeros if no keyframes
#             self.init_qpos = np.zeros(self.model.nq)
#         else:
#             self.init_qpos = np.array(init_qpos)

#         # Fixed goal pose [x, y, z, qx, qy, qz, qw]
#         self.goal_pose = np.array(goal_pose) if goal_pose is not None else None

#         # Identify end-effector site (must exist in XML as <site name="ee_site" ...>)
#         self.ee_site_id = mujoco.mj_name2id(
#             self.model, mujoco.mjtObj.mjOBJ_SITE, "ee_site"
#         )

#         # Observation: [qpos (nq), qvel (nv), ee_pose (7), goal_pose (7)]
#         obs_dim = self.model.nq + self.model.nv + 7 + 7 + 7
#         self.observation_space = spaces.Box(
#             -np.inf, np.inf, shape=(obs_dim,), dtype=np.float32
#         )

#         # Action: direct control of joint torques or velocities (size = nu)
#         self.action_space = spaces.Box(
#             -1.0, 1.0, shape=(self.model.nu,), dtype=np.float32
#         )

#         # Reward weights
#         # first reward: ë°œì‚°
#         self.alpha = 1.0    # position error weight
#         self.beta = 0.01    # velocity penalty
#         self.gamma = 0.001  # jerk penalty

#         # Episode limits
#         self.max_steps = 100 #100ì—ì„œ ë°”ê¿ˆ -> 100í–ˆì„ ì‹œ 245iteationì •ë„
#         self.step_count = 0
#         self.last_action = np.zeros(self.model.nu)

#         #ë Œë”ëŸ¬ ì´ˆê¸°í™”
#         self.viewer = None
#         self.renderer = None

#     def render(self, mode=None):
#         # human ëª¨ë“œ: ì°½ ë„ì›Œì„œ ì‹¤ì‹œê°„ ë³´ê¸°
#         if mode is None:
#             mode = self.render_mode

#         if mode == "human":
#             if self.viewer is None:
#                 from mujoco import viewer
#                 self.viewer = viewer.launch_passive(self.model, self.data)
#             self.viewer.sync()  # Sync viewer with data
            
#             '''
#             if not hasattr(self, "viewer"):
#                 from mujoco.viewer import MjViewer
#                 self.viewer = MjViewer(self.model)
#             self.viewer.render()
#             '''

#         # rgb_array ëª¨ë“œ: í”½ì…€ ë°°ì—´(í”„ë ˆì„) ë¦¬í„´
#         elif mode == "rgb_array":
#             # MuJoCo ì˜¤í”„ìŠ¤í¬ë¦° ë Œë” ì»¨í…ìŠ¤íŠ¸ ìƒì„±
#             if self.viewer is None:
#                 from mujoco import Renderer
#                 self.renderer = Renderer(self.model, 480, 480)
            
#             self.renderer.update_scene(self.data)
#             return self.renderer.render()  # (H, W, 3) uint8
#             '''
#                 self.viewer = viewer.launch_passive(self.model, self.data)
#             from mujoco.viewer import OffscreenRenderer
#             if not hasattr(self, "renderer"):
#                 # (model, width, height)
#                 self.renderer = OffscreenRenderer(self.model, 480, 480)
#             # í”½ì…€ ë²„í¼ ì½ì–´ì˜¤ê¸° (H, W, 3), uint8
#             img = self.renderer.render(self.data)
#             # MuJoCoëŠ” ì´ë¯¸ì§€ê°€ ë’¤ì§‘í˜€ ë‚˜ì˜µë‹ˆë‹¤. ìœ„ì•„ë˜ ë’¤ì§‘ì–´ì„œ ë°˜í™˜
#             return img
#             '''
#         else:
#             if mode is None:
#                 raise ValueError(f"Unknown render_mode: {mode}")


#     def reset(self):
#         # Reset joint positions and velocities
#         self.data.qpos[:] = self.init_qpos
#         self.data.qvel[:] = 0
#         mujoco.mj_forward(self.model, self.data)

#         # --- XML ì •ì˜ëœ site ë¡œë¶€í„° ì´ˆê¸° EE pose ì¶”ì¶œ ---
#         # lap_start ìœ„ì¹˜/ìì„¸ â†’ self.init_ee_pose ì— ì €ì¥
#         start_pos = self.data.site_xpos[self.start_site_id].copy()
#         quat_buf  = np.zeros(4, dtype=np.float64)
#         mujoco.mju_mat2Quat(quat_buf, self.data.site_xmat[self.start_site_id])
#         self.init_ee_pose = np.concatenate([start_pos, quat_buf])

#         # --- XML ì •ì˜ëœ site ë¡œë¶€í„° ëª©í‘œ pose ì„¤ì • ---
#         # lap_end ìœ„ì¹˜/ìì„¸ â†’ self.goal_pose ì— ì €ì¥
#         end_pos   = self.data.site_xpos[self.end_site_id].copy()
#         quat_buf2 = np.zeros(4, dtype=np.float64)
#         mujoco.mju_mat2Quat(quat_buf2, self.data.site_xmat[self.end_site_id])
#         self.goal_pose    = np.concatenate([end_pos, quat_buf2])

#         # # Sample a random goal if none provided
#         # if self.goal_pose is None:
#         #     # End-effector position
#         #     ee_pos = self.data.site_xpos[self.ee_site_id].copy()
#         #     # Convert rotation matrix to quaternion
#         #     mat = self.data.site_xmat[self.ee_site_id]
#         #     quat_buf = np.zeros(4, dtype=np.float64)
#         #     mujoco.mju_mat2Quat(quat_buf, mat)
#         #     ee_quat = quat_buf.copy()
#         #     pos = ee_pos + np.random.uniform(-0.1, 0.1, size=3)
#         #     quat = ee_quat
#         #     self.goal_pose = np.concatenate([pos, quat])

#         self.step_count = 0
#         self.last_action[:] = 0
#         return self._get_obs()

#     def _get_obs(self):
#         qpos = self.data.qpos.copy()
#         qvel = self.data.qvel.copy()
#         ee_pos = self.data.site_xpos[self.ee_site_id].copy()
#         # Convert rotation matrix to quaternion for observation
#         mat = self.data.site_xmat[self.ee_site_id]
#         quat_buf = np.zeros(4, dtype=np.float64)
#         mujoco.mju_mat2Quat(quat_buf, mat)
#         ee_quat = quat_buf.copy()
#         return np.concatenate([
#             qpos, qvel, ee_pos, ee_quat, self.init_ee_pose, self.goal_pose
#         ])

#     def step(self, action):
#         # Clip and apply action
#         action = np.clip(action, self.action_space.low, self.action_space.high)
#         self.data.ctrl[:] = action

#         # Step the simulation
#         mujoco.mj_step(self.model, self.data)

#         obs = self._get_obs()

#         # Compute reward
#         ee_pos = self.data.site_xpos[self.ee_site_id]
#         goal_pos = self.goal_pose[:3]
#         dist = np.linalg.norm(ee_pos - goal_pos)
#         vel = np.linalg.norm(self.data.qvel)
#         jerk = np.linalg.norm(action - self.last_action)
#         ''' ì²˜ìŒ reward -> RMSE
#         reward = - (
#             self.alpha * dist**2
#             + self.beta * vel**2
#             + self.gamma * jerk**2
#         )'''
#         # 2ë²ˆì§¸ reward -> clip reward
#         raw = - (
#             self.alpha * dist**2
#             + self.beta * vel**2
#             + self.gamma * jerk**2
#         )

#         #3ë²ˆì§¸ ì¶”ê°€ - ëª©í‘œ ë„ë‹¬ ì„±ê³µì‹œ +1ë³´ë„ˆìŠ¤
#         if dist < 10.0:
#             raw += 100.0
#         # 1.0 ë³´ë„ˆìŠ¤ëŠ” ëª©í‘œ ë„ë‹¬ì‹œë§Œ ì£¼ê³ , ê·¸ ì™¸ì—” -1.0 ~ 1.0 ì‚¬ì´ë¡œ ë³´ìƒ
#         reward = float(np.clip(raw, -1.0, 1.0))
#         # clip reward to [-1, 1] range

#         self.last_action = action.copy()
#         self.step_count += 1

#         done = bool(dist < 0.01 or self.step_count >= self.max_steps)
#         info = {"distance": float(dist)}
#         return obs, reward, done, info
#         #return super().close()
    
#     def close(self):
#         if self.viewer is not None:
#             self.viewer.close()
#             self.viewer = None
        
#         if self.renderer is not None:
#             self.renderer = None




#------------------------------------------------------------------------------------------------------------------
''' 
import gym
from gym import spaces
import numpy as np
import mujoco


class FrankaReachEnv(gym.Env):
    """Reach a target pose with the Franka Panda arm in MuJoCo."""
    metadata = {"render_modes": ["human", "rgb_array"]}

    # ------------------------------------------------------------------ #
    # ì´ˆê¸°í™”
    # ------------------------------------------------------------------ #
    def __init__(
        self,
        xml_path: str,
        init_qpos: np.ndarray | None = None,
        goal_pose: np.ndarray | None = None,
        render_mode: str | None = None,
    ):
        super().__init__()

        # MuJoCo ëª¨ë¸Â·ë°ì´í„°
        self.model = mujoco.MjModel.from_xml_path(xml_path)
        self.data  = mujoco.MjData(self.model)

        # ì´ˆê¸° ê´€ì ˆê° / ëª©í‘œ í¬ì¦ˆ
        self.init_qpos = np.zeros(self.model.nq) if init_qpos is None else np.array(init_qpos)
        self.goal_pose = None if goal_pose is None else np.array(goal_pose)

        # EE site id
        self.ee_site_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_SITE, "ee_site")

        # ê´€ì¸¡/í–‰ë™ ê³µê°„
        obs_dim = self.model.nq + self.model.nv + 7 + 7      # qpos+qvel+ee(7)+goal(7)
        self.observation_space = spaces.Box(-np.inf, np.inf, shape=(obs_dim,), dtype=np.float32)
        self.action_space      = spaces.Box(-1.0, 1.0, shape=(self.model.nu,), dtype=np.float32)

        # ë³´ìƒ ê³„ìˆ˜
        self.alpha, self.beta, self.gamma = 1.0, 0.01, 0.001

        # ì—í”¼ì†Œë“œ ì„¤ì •
        self.max_steps   = 100
        self.step_count  = 0
        self.last_action = np.zeros(self.model.nu)

        # --- ë Œë” ê´€ë ¨ ---
        self.render_mode = render_mode          # ì‚¬ìš©ìê°€ ë„˜ê¸´ ëª¨ë“œ ì €ì¥
        self._viewer     = None                 # human ëª¨ë“œ
        self._renderer   = None                 # rgb_array ëª¨ë“œ(ì§€ì—° ì´ˆê¸°í™”)

    # ------------------------------------------------------------------ #
    # ë Œë”ë§
    # ------------------------------------------------------------------ #
    def render(self, mode: str | None = None):
        mode = mode or self.render_mode or "rgb_array"   # default = rgb_array

        if mode == "human":
            if self._viewer is None:
                from mujoco.viewer import Viewer        # MuJoCo â‰¥ 3.1
                self._viewer = Viewer(self.model, self.data)
            self._viewer.render()
            return

        elif mode == "rgb_array":
            if self._renderer is None:
                self._renderer = mujoco.Renderer(self.model, 480, 480)
            self._renderer.update_scene(self.data)
            img = self._renderer.render()               # (H,W,3) uint8
            return img

        else:
            raise ValueError(f"Unknown render_mode: {mode}")

    # ------------------------------------------------------------------ #
    # Gym API
    # ------------------------------------------------------------------ #
    def reset(self, *, seed=None, options=None):
        super().reset(seed=seed)

        self.data.qpos[:] = self.init_qpos
        self.data.qvel[:] = 0
        mujoco.mj_forward(self.model, self.data)

        # ë¬´ì‘ìœ„ ëª©í‘œ
        if self.goal_pose is None:
            ee_pos = self.data.site_xpos[self.ee_site_id]
            quat   = np.zeros(4); mujoco.mju_mat2Quat(quat, self.data.site_xmat[self.ee_site_id])
            pos    = ee_pos + self.np_random.uniform(-0.1, 0.1, size=3)
            self.goal_pose = np.concatenate([pos, quat])

        self.step_count  = 0
        self.last_action = np.zeros_like(self.last_action)

        return self._get_obs()

    def step(self, action):
        # ì•¡ì…˜ ì ìš©
        action = np.clip(action, self.action_space.low, self.action_space.high)
        self.data.ctrl[:] = action
        mujoco.mj_step(self.model, self.data)

        # ê´€ì¸¡
        obs = self._get_obs()

        # ë³´ìƒ
        ee_pos   = self.data.site_xpos[self.ee_site_id]
        goal_pos = self.goal_pose[:3]
        dist  = np.linalg.norm(ee_pos - goal_pos)
        vel   = np.linalg.norm(self.data.qvel)
        jerk  = np.linalg.norm(action - self.last_action)

        raw_r = -(self.alpha*dist + self.beta*vel + self.gamma*jerk)
        if dist < 0.01:
            raw_r += 1.0                             # ëª©í‘œ ë„ë‹¬ ë³´ë„ˆìŠ¤
        reward = float(np.clip(raw_r, -1.0, 1.0))

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        self.last_action = action.copy()
        self.step_count += 1
        done = dist < 0.01 or self.step_count >= self.max_steps
        info = {}
        return obs, reward, done, info

    # ------------------------------------------------------------------ #
    # í—¬í¼
    # ------------------------------------------------------------------ #
    def _get_obs(self):
        qpos = self.data.qpos
        qvel = self.data.qvel

        ee_pos = self.data.site_xpos[self.ee_site_id]
        quat   = np.zeros(4); mujoco.mju_mat2Quat(quat, self.data.site_xmat[self.ee_site_id])

        return np.concatenate([qpos, qvel, ee_pos, quat, self.goal_pose])
'''